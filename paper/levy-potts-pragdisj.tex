\documentclass{article}

\input{preamble}

\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{Negotiating lexical uncertainty and expertise with disjunctive utterances}
\author{Roger Levy and Christopher Potts}
\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Communicating in language about language}\label{sec:introduction}

Natural languages are neither fixed across time nor identically
reproduced in all speakers, but rather continually renegotiated during
interactions \citep{Clark97}. Discourse participants accommodate to
each other's usage patterns \citep{Giles:Coupland:Coupland:1991}, form
temporarily lexical pacts to facilitate communication
\citep{Clark:Wilkes-Gibbs:1986,Brennan:Clark:1996}, and instruct each
other about their linguistic views. Some of this communication in
language about language is direct, as with explicit definitions like
\word{`oenophile' means wine lover}, but much of it arrives via
secondary pragmatic inferences, as when \word{X such as Y} conveys
that \word{X} subsumes \word{Y} \citep{Hearst92,SnowEtAl05}.

Disjunction supports what appear to be opposing inferences about
language. On the one hand, \word{X or Y} tends to convey that the
meanings of \word{X} and \word{Y} are presumed to be disjoint
\citep{Hurford:1974}, because the speaker holds such a view of the
lexicon or is worried that the listener might. This pressure to
exclusivize is robust enough to overcome even seemingly non-negotiable
aspects of the lexicon; a medical webpage warns ``If you still have
symptoms or severe blockage in your arteries, you may need angioplasty
or surgery'', sending a clear signal that angioplasty and surgery
are distinct options. Its continuation presupposes just that: 
``Having one of these procedures may save your leg''. The disjunction
might seem to be a needlessly verbose way of conveying the meaning of
the more general disjunct, but the costs could be worth paying in
virtue of the lexical side-effect of exclusivization.

On the other hand, disjunctions like \word{wine lover or oenophile}
can be used to convey that the two disjuncts are roughly synonymous
\citep{Horn89}, thereby providing secondary information that maximally
violates the pressure to exclusivize. This inference is more rarefied
than the exclusivization inference, but it can arise in a broad range
of contexts in which such definitional or identificational information
has social or communicative value. It is striking that the
definitional and exclusivization inferences are supported by a single
lexical item, and the puzzle deepens when we see that the empirical
picture is not a quirk of English, but rather one that we find in a
wide range of typologically and geographically diverse languages.

In this paper, we capture both of these classes of inference within a
single recursive Bayesian model of pragmatic reasoning. The model
finds its conceptual origins in \posscitet{Lewis69} work on signaling
systems and builds on ideas from iterated best response models
\citep{Jaeger:2007,Jaeger:2011,Franke09DISS} and more thoroughly
probabilistic variants of them
\citep{CamererHo:2004,Frank:Goodman:2012}. The crucial feature of our
model is that it lets discourse participants communicate, not just
about the world, but also about the language they are using
\citep{Bergen:Goodman:Levy:2012,bergen-levy-goodman:2014}: the
speaker's intentions in production are characterized in terms of both
world information and linguistic information, and the listener's
pragmatic reasoning is cast as a problem of joint inference about the
speaker's intended meaning and preferred lexicon
\citep{Smith:Goodman:Frank:2013}. We show that, within this model,
both exclusivization and definitional inferences arise naturally from
the expected semantic content of disjunction, depending on contextual
parameters relating to speaker expertise, listener malleability, and
information contained in the common ground. The model thus offers a
genuinely pragmatic account of these inferences as well as
characterizations of their stability and their communicative value.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Lexical side-effects from disjunction}\label{sec:data}

This section offers more expanded characterizations of the
exclusivization and definitional uses of disjunction. We seek to
identify features of the contexts of use, and the resulting
inferences, that are shared across the two, and to begin to pinpoint
the features of the context that steer speakers and listeners toward
one or the other. These findings inform the modeling we describe in
\dashsecref{sec:model}{sec:analysis}.


%=====================================================================

\subsection{Hurfordian perceptions and intentions}\label{sec:data:overlapping}

\posscitet{Hurford:1974} generalization (HG) is a direct statement of
the overall communicative pressure to treat disjuncts as exclusive:
%
\begin{examples}
\item\label{hg}%
  ``The joining of two sentences by \word{or} is unacceptable if one
  sentence entails the other; otherwise the use of \word{or} is
  acceptable.'' (p.~410)
\end{examples}
%
The generalization is stated in terms of sentences, but Hurford's
examples, given in \eg{hex} with his original judgments, make it clear
that he intends it to hold for sub-sentential disjuncts as well (he is
likely assuming conjunction reduction):
%
\begin{examples}
\item\label{hex}
  \begin{examples}
  \item Ivan is an American or Russian.
  \item The painting is of a man or a woman.
  \item The value of $x$ is greater than or equal to 6.
  \item\label{ex-bad1}\bad John is an American or Californian.
  \item\label{ex-bad2}\bad The painting is of a man or a bachelor.
  \item\label{ex-bad3}\bad The value of $x$ is greater than or not equal to 6.
  \end{examples}
\end{examples}

\citeauthor{Hurford:1974} uses HG to probe the nature and distribution
of conversational implicatures (see also
\citealt{Gazdar79b,ChierchiaFoxSpector08}), and \citet{Singh:2008}
extends it to cases in which the disjuncts are merely overlapping. We
endorse the guiding insight behind these accounts but reject the
assumption that HG violations reliably lead to, or even correlate
with, unacceptability or ungrammaticality. Disjunctions of apparently
entailing phrases are routine; all of the examples marked as
ungrammatical in \eg{hex} are easy to find in fluent English text on
the Web:
%
\begin{examples}
\item\label{hex-good}
  \begin{examples}
  %\item ``How much does the average american or californian pay every
  %  year toward state taxes?''
  \item ``\ldots and we trust that some of our \highlight{American or Californian}
    friends will tell us something of its growth of flower and fruit
    in its native habitats''
  \item ``It doesn't matter if you ask \highlight{a boy or a man or a bachelor or
    even a husband}, \ldots''
  \item ``the effect was \highlight{greater than, or not equal to,} the cause.''
  \end{examples}
\end{examples}

We have collected a large corpus of apparent counterexamples,
available at \url{http://goo.gl/VAGqnB}. Here is a small sample from
that corpus:
%
\begin{examples}
\item\label{ourcorpus} 
  %
  \begin{examples}
    %%%%%%%%%% X < Y
  \item Stop discrimination of an \highlight{applicant or person} due
    to their tattoos.
  \item Promptly report any \highlight{accident or occurrence}.
  \item The anchor will lie on the bottom and the \highlight{canoe or
      boat} will be held by the stream's current.
  \item ``As an \highlight{actor or performer}, you are always worried
    about what the next job's going to be,'' Hensley says.
    %%%%%%%%%%  X > Y
  \item After the loss of the \highlight{animal or pet}, there are
    further coping strategies available for the grieving individual.
  \item Bush was captured slyly removing \highlight{a candy or gum}
    from his mouth.
  \item Heroic is not a word one uses often without embarrassment to
    describe a \highlight{writer or playwright} \ldots
  \item But he never attended school during his senior year, never
    attended a \highlight{party or prom}.
  \end{examples}
\end{examples}
%
The dataset includes XXXX cases where the left disjunct entails the
right, and XXXX in which the right entails the left.  In addition, we
conjecture that, for any two nouns $N_{1}$ and $N_{2}$ one believes to
be in an overlap or proper entailment relation, it will generally be
possible to find a context in which ``$N_{1}$ or $N_{2}$'' and
``$N_{2}$ or $N_{1}$'' are felicitous, and Web searches will generally
yield examples.

Eager for a more rigorous assessment of the prevalence of HG-violating
disjunctions, we conducted a simple experiment. First, we obtained a
list of countries and their capitals and, to keep the search procedure
simple, extracted the subset in which the country and capital both
have single word names. Second, we search for `\emph{country or
  capital}' and `\emph{capital or country}' in the Google Books
N-grams data, restricting attention to books after 1960 to try to
avoid some of the encoding difficulties that plague earlier texts in
that corpus. The results of this experiment are summarized in
\tabref{fig:capitals} (the raw data are available at the website
for this paper).

\begin{figure}[tp]
  \centering
  \begin{subtable}{0.45\textwidth}
    \begin{tabular}[c]{r r r}    
      \toprule
      & Attested  \\
      \midrule
      \emph{country or capital}  & N/TOTAL \\
      \emph{capital or country}  & N/TOTAL \\
      \bottomrule
    \end{tabular}
    \caption{Percentage attested limiting to cases where both
      \emph{country} and \emph{capital} are found in the data
      separately.}
  \end{subtable}
  \hfill
  \begin{subtable}{0.45\textwidth}
    \begin{tabular}[c]{l}
      Barplot of \emph{country or capital} \\
      and \emph{capital or country}\\
      Summary based on t-test or similar.
    \end{tabular}
    \caption{Relative frequencies of the two orders. A Wilcoxon
      signed-rank test XXXX.}      
  \end{subtable}
  \caption{Countries and their capitals in disjunctions.}
  \label{fig:capitals}
\end{figure}

Of course, one would like to extend the above experiment to a broader
and more diverse sample of the lexicon, to get a clearer picture of
the prevalance of these disjunctions and to probe ordering preferences
that might relate to the direction of entailment. However, we do not
see a way to do this accurately. The primary obstacle is, we believe,
an important property of the phenomenon itself: judgments about
lexical entailment are inherently very messy because of the flexible
ways in which people refine meanings in context. As a result, there
often isn't a single objective answer to the question of whether two
disjuncts stand in an an entailment relation. For instance, whereas
\subeg{exclusive}{franceorparis} leaves little room to negotiate the
meaning of the terms in any clear sense,
\subeg{exclusive}{churchorsynagogue} is much less clear.
%
\begin{examples}
\item\label{exclusive}
  \begin{examples}
  \item\label{franceorparis} ``The nuptials will take place in either
    France or Paris.''
  \item\label{churchorsynagogue} ``In 1940, 37 percent of us had gone
    to a church or synagogue in the last week.''
  \end{examples}
\end{examples}
%
Some speakers have firm judgments that \word{church} and
\word{synagogue} exclude each other, making this case clearly
HG-respecting. However, it is easy to find uses of the phrase
``synagogues and other churches'', which presuppose that a synagogue
is a kind of church. And we should take care even with our assertion
that \word{France} and \word{Paris} invariably stand in an entailment
relation. In contexts where France is being construed in terms of its
countryside, or Paris in terms of its particular urban charms,
\word{France} could come to mean something more like `Paris outside of
France'. The important thing for our purposes is that the insight
behind HG shines through this uncertainty: no matter what one's
initial view of the lexicon is, disjunctions like \word{X or Y} make
salient a construal of the lexicon in which the disjuncts are
semantically disjoint. The speaker will be perceived as endorsing such
a view, at least for the current conversational exchange, and the
listener can either adopt that assumption or push back.

This lexical uncertainty motivates our own explanation for why
speakers utter HG-violating disjunctions. In broad terms, we say that
such examples convey that the speaker is treating the two terms as
exclusive. There are many potential motivations for this. Perhaps the
most mundane is that the speaker simply lexicalizes the two terms as
exclusive. The disjunction is likely to be easily justified in such
cases, as it might be the most efficient and direct way of identifying
the union of the two terms.

More interesting are cases in which the speaker's disjunction seems to
be part of an attempt to manage the listener's inferences. For
instance, the speaker who uses the phrase \word{swimwear or bikini}
might be concerned that using \word{swimwear} alone will trigger an
ad-hoc scalar implicature \citep{Hirschberg85} excluding the salient
subkind of bikinis. \citet{Chemla-HurfordCounts} studies this class of
inferences, presenting suggestive evidence that the frequency of
disjunctions \word{X or Y} (\word{X} subsuming \word{Y}) is positively
correlated with the likelihood that \word{X} conversationally
implicates \word{not Y} as estimated by the experimental results of
\citet{vanTiel-etal:2013}.  \citeauthor{Chemla-HurfordCounts}'s
experiment relies on the page counts in Google search results, which
are notoriously unreliable \citep{Liberman:2005}, so we reproduced his
main finding using Google Books data \citep{Michel-etal:2011} and a
more direct method for assessing the relative frequency of \word{X or
  Y}. \Figref{fig:chemla} summarizes this experiment, and the raw data
are available at the website associated with this paper.

\begin{figure}[tp]
  \centering
  \includegraphics[width=1\textwidth]{fig/chemla}
  \caption{A corpus study inspired by \citet{Chemla-HurfordCounts} on
    the relationship between the probability that \word{X}
    conversationally implicates \word{not-Y} and the 
    frequency of disjunctions \word{X or Y} relative to 
    the more general term \word{X}. A linear regression (blue line)
    suggests a modest but robust correlation (coef = $6.580^{-06}$; 
    standard error $3.095^{-06}$; $p = 0.04$; $R^{2} = 0.08$).}
  \label{fig:chemla}
\end{figure}

The speaker might wish to block a subtly different kind of implicature
in a situation in which a general term is prone to being limited to a
prototypical subkind \citep{Levinson00}. For instance, at a busy
marina in water-skiing country, \word{boat} might come to identify
just motorboats. In that case, a speaker might use \word{boat or
  canoe} or \word{boat or kayak} to ensure that these non-motorized
cases are included in a rule, lest people assume that these rarer
kinds of boat are exempt.

In both of these implicature-blocking scenarios, the speaker is
concerned that the general term \word{X} will be construed as
$\sem{\word{X}}-\sem{\word{Y}}$ for some \word{Y}. Thus, \word{X} or
\word{Y} is a hedge against the possibility that the listener's view
of language is such that, at least in the current context,
$\sem{\word{X}} \cap \sem{\word{Y}} = \emptyset$, or, as in
\posscitet{Singh:2008} account,
$\sem{\word{Y}} \nsubseteq \sem{\word{X}}$. This is a defensive
position; the speaker's own lexicon might allow her to use just the
general term to convey her intentions, but she is concerned that the
listener will arrive at a different conclusion. The costs of
disjunction are therefore worth paying even if it adds no new
information given the speaker's lexicon.  However, the speaker can
play a more active role as well, using disjunctions to instruct the
listener about the right lexicon. Our \secref{sec:introduction}
example containing ``you may need angioplasty or surgery'' seems to be
an instance of this: the disjunction conveys secondary information
that \word{angioplasty} and \word{surgery} will be treated as separate
options in the current discourse. If HG were adopted as an explicit
theoretical constraint, then the possibility of doing this would more
or less follow --- we just require the additional premise that the
listener is charitable and so will try to find an acceptable construal
of the utterance. The model we develop in \secref{sec:model} also
supports this kind of reasoning, but it requires no independent
statement of HG.
 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Definitional and identification}\label{sec:data:definitional}

% A Geological History of Manhattan or New York Island

Disjunctions like \word{wine lover or oenophile} seem to fly in the
face of the Hurfordian pressure reviewed just above. Rather than
avoiding overlap, they seem to embrace it, conveying something
approximating identity. These readings seem to be more contextually
restricted even than HG-violating disjunctions, and speakers often
(but not always) signal them with ad hoc prosody, italics, quotation
marks, and other devices. However, it would be a mistake to dismiss
them as an idiosyncracy of English, since these uses are widely
attested in typologically diverse languages (we have examples from
Chinese, German, Hebrew, Ilokano, Japanese, Russian, and
Tagalog). Even languages that seem to have a dedicated `definitional'
or `metalinguistic \word{or}' (e.g., Finnish, Italian) seem also to
allow the regular \word{or} to play this role.

For our purposes, the most important property of these uses is that
they convey a meaning that is secondary to the main content of the
utterance --- an extreme instance of a meaning that is not at issue
\citep{Tonhauser-etal:2011}. This contrasts with overt definitions
like \word{oenophile means wine-lover} or \word{oenophile: wine-lover}
(in a dictionary context).  We think it is no accident that another
strategy for conveying definitional information in this non-asserted,
taken-for-granted manner is via apposition, as in \word{oenophile
  (`wine-lover')} \citep{Potts05BOOK}. In this respect, the relevant
inference resembles the exclusivization pressure identified by HG:
both seem to emerge as side-effects rather than normal outputs. In our
model (\secref{sec:model}), both are in turn characterized as
inferences about the lexicon rather than about the state of the world.

In addition, as with disjunct exclusivization, the relevant lexical
inference might be temporary. For instance, phrases like
\word{Internet or computer network} seem to use the second phrase as a
rough-and-ready way of helping the listener bootstrap towards an
understanding of what the Internet is like. Even our wine-lover
example seems to involve only approximate synonymy; \word{wine lover
  or oenophile} seems apt in a context in which the speaker wishes to
use \word{oenophile} to elevate the concept to something more specific
(or pretentious) than \word{wine lover} picks out. Similarly, the book
title \emph{A Geological History of Manhattan or New York Island}
seems to identify Manhattan and New York Island while at the same time
acknowdging the different histories and connotations of the two
disjunct terms.

The speaker's motivations for using definitional disjunction are quite
varied. Such readings seem to arise most easily when the speaker is
mutually and publicly known to be a expert in the domain covered by
the terms and the listener is mutually and publicly known to be
inexpert in that area. In such cases, the speaker can use the
disjunction to convey information about her preferred lexicon, fairly
certain that the listener will be receptive. However, while speaker
expertise seems to be a genuine prerequisite, the listener's knowledge
seems to impose much less.  We find natural uses of this strategy when
there is no direct information about the listener, but rather just a
general assumption that one of the terms is relatively unknown. For
instance, a newspaper article might contain \word{wine lover or
  oenophile} without presuming that all its readers are ignorant;
rather, such a use would seem to presuppose only that \word{oenophile}
is relatively unknown.  Finally, at the the other end of the extreme,
the listener might actually be presumed to know the term, but the
speaker sees social value in conveying that she shares this view. This
could be because the speaker would like to display expertise, as when
an ambitious undergraduate seeks to convey competence to a professor.
The uses also arise when the speaker and listener might both be
experts in the domain and see a value (jointly or just in the current
speaker's eyes) of using a word in a specialized sense in order to
name a concept efficiently (e.g. \word{What motivates the snobbish
  wine lover or `oenophile' and how does he differ from the casuaal
  drinker?}).

For these reasons, we retreat to a more basic characterization: the
discourse participants must have a mutual interest in communicating
about their language and arriving at a refined, perhaps
context-specific joint understanding of it, and there should be a
background assumption that speaker and listener are willing to
coordinate on the lexicon that the speaker seems to be using. In
addition, we hypothesize that the cost of using a disjunction must be
fairly small, all things considered, else it is hard to see how the
speaker could justify using a disjunction \word{X or Y} to convey
simply $\sem{\word{X}}$. At any rate, whatever the costs of using the
verbose form, they must be worth paying in virtue of the benefits of
identifying (for the purposes of the current talk exchange) the
meanings of the two disjuncts.


% There can be uncertainty about whether the listener should regard
% the disjunction as definitional. We might enter into a blurry area
% in which the listener is struggling to figure out whether the
% intentions are definitional or exclusivizing. This can persist even
% when one of the words is unknown to the listener. In such cases, the
% disjunctive meaning is just extremely general and will not do
% justice to the speaker's intentions.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{A model of communication with anxious experts}\label{sec:model}

The model finds its conceptual origins in \posscitet{Lewis69} work on
signaling systems and builds on ideas from the iterated best response
models of \citet{Jaeger:2007,Jaeger:2011} and
\citet{Franke09DISS}. Models of this form have been shown to achieve
tight correlations with experimental data (e.g.,
\citealt{Frank:Goodman:2012}) and to offer unified, precise account of
a wide range of conversational implicatures
\citep{bergen-levy-goodman:2014}.

\begin{examples}
\item\label{model}
  \begin{examples}
  \item\label{states}%
    $\States$ is a set of states (worlds, referents, propositions, etc.).
  \item\label{messages}%
    $\Messages$ is a set of messages containing designated `null' message $\nullmsg$.
  \item\label{lex}%
    $\Lex: \Messages \mapsto \wp(\States)$ is a semantic interpretation function. 
    $\Lex'(\nullmsg) = \States$
  \item\label{prior}%
    $\Prior : \States \mapsto [0,1]$ is a prior probability
    distribution over states.    
  \item\label{costs}%
    $\Costs : \Messages \mapsto \Reals$ is a cost function on messages.
    $\Costs(\nullmsg) > \Costs(\msg), {\forall \msg \in \Messages{-}\set{\nullmsg}}$
  \item\label{lexset}%
      $\LexSet = \set{\Lex' : \Lex'(\nullmsg) = \States \text{ and } 
      \forall \msg \in \Messages{-}\set{\nullmsg}, 
      \Lex'(\msg) \neq \emptyset \text{ and } 
      \Lex'(\msg) \subseteq \Lex(\msg)}$
  \item\label{LexPrior}% 
    $\LexPrior : \LexSet \mapsto [0,1]$ is a prior
    probability distribution over lexica.
  \item\label{temps}%
    Real-valued learning-rate parameters $\alpha$, $\beta$, and
    $\gamma$
  \end{examples}
\end{examples}

Clauses~\dashsubeg{model}{states}{costs} define a signaling system. We
imagine that the speaker and listener are playing a game in which the
speaker privately observes a state $\state \in \States$ and produces a
message $\msg \in \Messages$ on that basis, given the context defined
by the signaling system. The listener then uses $\msg$ to guess a
state $\state' \in \States$, and the communication succeeds just in
case $\state = \state'$.

We assume that the set of messages includes a designated null message
$\nullmsg$ that is true in all states in all lexica. It can be thought
of as a catcha-all for the numerous messages in the language that are
not discriminating in the context defined by the signaling system.  No
matter how big and complex our examples grow, we assume that such a
message is always justified. It also helps ensure that various
calculations in the model are well-defined \citep{Jaeger:2011}.

Throughout this basic game, the two players assume that the lexicon is
fixed and mutually, publicly known. In \subeg{model}{lexset}, we see a
first retreat from this assumption. There, we define a set of
refinements of the origianl $\Lex$. In a sense, this defines the space
of pragmatic enrichment allowed by the fixed conventions. Here, we
restrict the lexica to that are refinements of the orginal and assign
non-empty denotations to every message. This is not a deep assumption
of the model, though. One could, for instance, define a much larger
space of lexica and then use the prior $\LexPrior$ to structure
that space with respect to the agents we define just below.

The parameters in \subeg{model}{temps} control the relative weighting
of information about the world, information about the lexicon, and
information about the message costs. We further explicate these
parameters when defining our agents just below.


Our model defines an intuitive hierarchy of agents. The most basic are
$\listenerZero$, $\speakerOne$, and $\listenerOne$. These agents
reason in terms of a fixed lexicon $\Lex$, which, for our purposes,
can be thought of as a standard interpretation function. 

The starting point is $\listenerZero$. This agent simply turns $\Lex$
into a probabilistic formulation that can be used for decision making
in the presence of communucative indeterminacy. The first term just
defines an even distribution over all of the true states $\state$ for
the given message $\msg$, and the second term incorporates the
contextual prior distribution over states $\Prior$.

\begin{examples}
\item\label{l0}%
    $\listenerZero(\state \given \msg, \Lex) \propto
    \frac{\Indicator(\state \in \Lex(\msg))}{|\Lex(\msg)|}
    \Prior(\state)$
\end{examples}

This agent is pragmatic only insofar as it incorporates the contextual 
prior into (a distribution derived from) the truth conditions. Richer
pragmatic inferences start to emerge as soon as we envision a speaker
who can reason in terms of this listener and plan its utterances 
accordingly. The minimal such speaker is $\speakerOne$, which is 
defined recursively in terms of $\listenerZero$:
%
\begin{examples}
  \item\label{s1}% 
    $\speakerOne(\msg \given \state, \Lex) \propto
    \exp
    \left(
      \log\left(\alpha\,\listenerZero(\state \given \msg, \Lex) \right)
      - 
      \gamma\,\Costs(\msg)
    \right)$
\end{examples}
%
Ths definition is a bit cumbersome because of the need to ensure that
all values are positive. At its heart this agent is parallel to
$\listenerZero$ in that it combines a conditional distribution and a
piece of contextual information --- now costs on messages. The result
is a distribution that one can imagaine serving as the basis for
decision making: given that the agent would like to convey that a
certain state holds, which message will do that most effectively for
the listener? The two positive, real-valued parameters $\alpha$ and
$\gamma$ control the rate of learning, in a manner similar to
temperature parameters in reenforcement learning models.

The first pragmatic listener is $\listenerOne$. It is parallel to
$\listenerZero$ except that it reasons, not in terms of the original
lexicon, but rather in terms of $\speakerOne$ reasining about
$\listenerZero$ reasoning about the lexicon:
%
\begin{examples}
  \item\label{l1}% 
    $\listenerOne(\state \given \msg, \Lex) \propto 
    \speakerOne(\msg \given \state, \Lex)
    \Prior(\state)$
\end{examples}

This basic model suffices to derive a range of implicatures involving
context-dependent reasoning about specific terms and their general
counterparts. Broadly speaking, such models can achieve variations on
the theme that using a general term will tend to convey that its
specific counterpart is inappropriate, creating scalar implicatures.
The moadels have been shown to achieve good quantitative fits to
experimental results.  One can also immediately generalize
$\listenerOne$ and $\speakerOne$ to allow them to recursively respond
to each other, which generally has the effect of soldifying
implicatures and predicting new ones that speakers seem less inclined
to perceive \citep{Vogel-etal:2014}.

The generalization we propose is diffent. Rather than defining further
recursion based on these lexicon-specific agents, we now open up the
model to allow for the possibility that the discourse participants are
negotiating not just what information they exchange but also what
language they use to exchange it. Our $\ListenerK$ achieves this on
the listener side; it is based on the social-anxiety listener of
\citet{Smith:Goodman:Frank:2013}.
%
\begin{examples}
  \item\label{Lk}%
    \setlength{\arraycolsep}{2pt}%  
    $\mspace{-4mu}\begin{array}[t]{r c l}
      \ListenerK(\state, \Lex \given \msg) 
      &=&
      \sum_{\Lex \in \LexSet} \listenerOne(\state \given \msg, \Lex) \ListenerK(\Lex \given \msg) 
      \\[1ex]
      \ListenerK(\Lex \given \msg) 
      &\propto& 
      \Prior(\Lex) \sum_{\state\in\States} \SpeakerK(\msg \given \state)\Prior(\state)
    \end{array}$
\end{examples}
%
In terms of its form, the most important feature of this model is that
it makes joint inferences about states and lexica; rather than taking
the lexicon as given, it tries to make guesses about this based on the
speaker's message. The second term models this information
specifically by characterizing the extent to which the speaker's
message discriminates among lexica. This listener is responding to
$\listenerOne$ in sense that it sums over all of the
$\listenerOne(\state\given\msg\Lex)$ inferences for all lexica.
  
Our speaker agent responds to this sophisticated joint listener.  We
assume that the speaker does have a specific lexicon in mind.  More
precisely, this speaker is taken to observe state--lexicon pairs and
produce messages on that basis. In broad terms, it is similar to
$\speakerOne$ except that it includes a new term
$\ListenerK[k-1](\Lex\given\msg)$.

\begin{examples}  
  \item\label{Sk}%
    $\SpeakerK(\msg \given \state, \Lex) \propto 
    \exp
    \left(
      \log
      \left(\alpha\,\ListenerK[k-1](\state \given \msg, \Lex)\right)
      - 
      \beta \log\left(\ListenerK[k-1](\Lex\given\msg)\right)
      -
      \gamma\,\Costs(\msg)
    \right)$
\end{examples}


The above can be unwieldy to think about because of the lexical
inferences. To return to the intuitive picture of simple
state/message signaling, we can sum over lexica, as in 
\eg{lisnorm} and \eg{spknorm}. Both provide useful summaries
of the model's predictions.
%
\begin{examples}
\item\label{lisnorm}% 
  $\ListenerK(\state \given \msg)  = 
  \sum_{\Lex \in \LexSet} \ListenerK(\state, \Lex \given \msg)$
\item\label{spknorm}% 
  $\SpeakerK(\msg \given \state) \propto 
  \sum_{\Lex \in \LexSet} \SpeakerK(\msg \given \state, \Lex)
  \LexPrior(\Lex)$
\end{examples}

\begin{figure}[htp]
  \centering
  \setlength{\tabcolsep}{2pt}
  \begin{tabular}[c]{ *{15}{c} }
   $\listenerZero$ && $\listenerOne$  & $\rightarrow$ & $\ListenerK[1]$ && $\ListenerK[2]$ && $\ListenerK[3]$\\
   \multicolumn{3}{c}{$\searrow$ $\nearrow$} &&\multicolumn{5}{c}{$\searrow$ $\nearrow$ $\searrow$ $\nearrow$ $\searrow$}\\
             & $\speakerOne$ &&&& $\SpeakerK[2]$ && $\SpeakerK[3]$ && $\ldots$   
  \end{tabular}
  \caption{Summary of model structure}
  \label{fig:modstruc}
\end{figure}

%=====================================================================

\section{Scalar implicature of disjunction}\label{sec:scalar-disj}

This initial example is meant to show how the model works and to show
that we can capture the core scalar implicature that disjunction is
best known for.

\begin{examples}
\item 
  \begin{examples}
  \item 
    $\Lex$:
    $\begin{array}[c]{r *{4}{c}}
      \toprule
                    & p      & q      & p \word{ or } q & p \word{ and } q \\
      \midrule
      w_{1}          & \True    & \False & \True     & \False \\
      w_{2}          & \True   & \True  & \True     & \True  \\
      w_{3}          & \False  & \True  & \True     & \False 
      \\[2ex]
      w_{1} \vee w_{2} & \True & \False & \True & \False \\
      w_{1} \vee w_{3} & \False & \False & \True  & \False \\ 
      w_{2} \vee w_{3} & \False & \True & \True  & \False \\
      w_{1} \vee w_{2} \vee w_{3} & \False & \False & \True  & \False 
      \\[2ex]
      w_{1} \wedge w_{2}  & \True  & \True & \True & \True\\
      w_{1} \wedge w_{3}  & \True  & \True & \True & \False\\
      w_{2} \wedge w_{3}  & \True  & \True & \True & \True\\
      w_{1} \wedge w_{2} \wedge w_{3} & \True  & \True & \True & \True\\
      \bottomrule
    \end{array}$
  \item $\alpha = 3$ (it's important that this be high for exclusivization)
  \item Disjunction cost: $0.1$; $\beta = 1$; $\gamma = 1$  
  \end{examples}
\end{examples}

\noindent
\begin{minipage}[c]{0.48\linewidth}
  \begin{examples}
  \item $\ListenerK[3]$ after marginalization 
    
    \vspace{-4pt}

     $\mspace{-90mu}$
     \includegraphics[width=1.2\textwidth]{fig/scalardisj-expertise-listener-marginalized}
  \end{examples}
\end{minipage}
\hfill
\begin{minipage}[c]{0.48\linewidth}
  \begin{examples}
  \item $\SpeakerK[3]$ summed over lexica
    
    \vspace{-4pt}

    $\mspace{-70mu}$
    \includegraphics[width=1.2\textwidth]{fig/scalardisj-expertise-speaker-lexsum}
  \end{examples}
\end{minipage}

\begin{examples}
\item Summary of the above via max associations:

  \begin{minipage}[t]{0.45\linewidth}
    Listener

    \vspace{-2pt}
  
    $\begin{array}[t]{r @{ \ \Rightarrow \ } l}
      \toprule
      \text{Message} & \text{Inference} \\
      \midrule
      p &  w_{1}\\
      q & w_{3} \\
      p \word{ or } q & w_{1} \vee w_{3} \\
      \bottomrule
    \end{array}$
  \end{minipage}
  \hfill
  \begin{minipage}[t]{0.45\linewidth}
    Speaker

    \vspace{-2pt}

    $\begin{array}[t]{r @{ \ \Rightarrow \ } l}
      \toprule
      \text{Observation} & \text{Best message} \\
      \midrule
      w_{1} & p \\
      w_{2} & \set{p, q} \\
      w_{3} & q  \\      
      w_{1} \vee w_{3} & p \word{ or } q \\
      \bottomrule
    \end{array}$
  \end{minipage}    
\end{examples}

%=====================================================================

\section{Analysis}\label{sec:analysis}


Throughout, we
assume that the communcative setting is governed by a few overarching
principles: 

 communication about language is at least somewhat
important to the discourse participants; 

Contextual requirements: 

\begin{examples}
\item The speaker is mutually and publicly known to be an expert in
  the relevant domain.
  
\item The speaker is mutually and publicly known to believe the
  listener to be a non-expert in the relevant domain.
  
\item The speaker is mutually and publically known to have an
  interest in conveying information about the language itself,
  even if this communicative inefficiency in terms of the 
  information conveyed about the world.
\end{examples}

\begin{examples}
\item Throughout, let \word{X} be the unknown term.

\item From the listener's perspective, we are concerned to see when
  \word{A or X} gives rise to the inference that $\sem{A} \cap \sem{X}
  = \emptyset$ (Hurfordian reading) and when \word{A or X} gives rise
  to the inference that $\sem{A} = \sem{X}$ (definitional).
\end{examples}

\newcommand{\smallhurfordlex}[3]{
  \left[
    \begin{array}[c]{l@{ \ \mapsto \ }r l@{ \ \mapsto \ }r l@{ \ \mapsto \ }r}
      A & \set{#1} &
      B & \set{#2} &
      X & \set{#3}
    \end{array}
  \right]}

%=====================================================================

\subsection{Subsumptive disjunctions}\label{sec:analysis:subsumptive}

\begin{examples}
\item From the listener's perspective, we are concerned to see when
  \word{A or X} gives rise to the lexical inference that $\sem{A} \cap
  \sem{X} = \emptyset$. We assume that the overall meaning will be
  $\sem{A} \cup \sem{X}$, i.e., that the relevant information concerns
  just the lexical inference.

\item Here's a scenario in which the exclusivization inference arises:

  \begin{examples}
  \item $\States = \set{w_{1}, w_{2}, w_{3}}$;  $\Messages = \set{A, B, X}$
  \item $\Lex = [A \mapsto \set{w_{1}}, B \mapsto \set{w_{2}}, X \mapsto \set{w_{1}, w_{2}}]$
  \item Priors are flat. $\alpha = 2$; $\beta = 1$; $\gamma = 1$. Disjunction cost: $1$. $n = 3$. 
  \end{examples}

\item The listener's best guess inference, upon hearing \word{A or X},
  is that the speaker's state is $w_{1} \vee w_{2}$ and that the
  lexicon is the one where $A$ and $X$ are disjoint. Here's the joint
  probability table:
  \[
  \renewcommand{\arraystretch}{2}
  \begin{array}[c]{l r r r}
    \toprule
            & w_{1} & w_{2} & w_{1} \vee w_{2} \\
    \midrule
    \smallhurfordlex{w_{1}}{w_{2}}{w_{1}, w_{2}} & 0 & 0 & 0.16 \\
    \smallhurfordlex{w_{1}}{w_{2}}{w_{2}} & 0 & 0 & \graycell{0.47} \\
    \smallhurfordlex{w_{1}}{w_{2}}{w_{1}} & 0 & 0 & 0.38 \\
    \bottomrule
  \end{array}
  \]

\item The above parameters deliver the same result with the the
  lexicon size increased (same pattern: each lexical item denotes its
  own world and $X$ denotes the union of all the worlds). This is
  unwieldy to visualize, but the important thing is that the best
  lexicon is always a Hurfordian one. Here's the best lexical
  inference with five atomic messages and four atomic states:
  \[
  \left[
    \begin{array}[c]{l@{ \ \mapsto \ }l}
      A & \set{w_{1}} \\
      B & \set{w_{2}} \\
      C & \set{w_{3}} \\
      D & \set{w_{4}} \\
      X & \set{w_{2}, w_{3}, w_{4}}
    \end{array}
  \right]
  \]
  This is the `minimal' Hurfordian lexicon: the listener doesn't infer
  anything about $X$ except that it is disjoint from $A$.    
\end{examples}

%=====================================================================

\subsection{Definitional disjunctions}\label{sec:analysis:definitional}

\begin{examples}
\item From the listener's perspective, we are concerned to see when
  \word{A or X} is interpreted as equivalent to $\sem{A}$.

\item From the speaker's perspective, we want to know what happens
  when the speaker favors a lexicon in which $\sem{A}=\sem{X}$ and
  observes a state equivalent to the literal meaning of $\sem{A}$.
  When will such a speaker produce \word{A or X}.

\item Here's an example in which these two perspectives are
  complementary. To achieve it, we have to substantially raise
  $\alpha$ and $\beta$ and lower disjunction costs:

  \begin{examples}
  \item $\States = \set{w_{1}, w_{2}, w_{3}}$;  $\Messages = \set{A, B, X}$
  \item $\Lex = [A \mapsto \set{w_{1}}, B \mapsto \set{w_{2}}, X \mapsto \set{w_{1}, w_{2}}]$
  \item Priors are flat. $\alpha = 5$; $\beta = 7$; $\gamma = 1$. Disjunction cost: $0.01$. $n = 3$. 
  \end{examples}

\item The listener's best guess inference, upon hearing \word{A or X},
  is that the speaker's state is $w_{1} \vee w_{2}$ and that the
  lexicon is $\Lex_{1}$. Here's the joint probability table:
  \[
  \renewcommand{\arraystretch}{2}
  \begin{array}[c]{l r r r}
    \toprule
            & w_{1} & w_{2} & w_{1} \vee w_{2} \\
    \midrule
    \smallhurfordlex{w_{1}}{w_{2}}{w_{1}, w_{2}} & 0 & 0 & 0 \\
    \smallhurfordlex{w_{1}}{w_{2}}{w_{2}}        & 0 & 0 & 0 \\
    \smallhurfordlex{w_{1}}{w_{2}}{w_{1}}        & \graycell{0.88} & 0 & 0.12\\
    \bottomrule
  \end{array}
  \]

\item The above parameters deliver the same result with the the
  lexicon size increased. Here's the best lexical inference with five
  atomic messages and four atomic states --- in this way, the 
  listener has learned that $A$ and $X$ are synonymous:
  \[
  \left[
    \begin{array}[c]{l@{ \ \mapsto \ }l}
      A & \set{w_{1}} \\
      B & \set{w_{2}} \\
      C & \set{w_{3}} \\
      D & \set{w_{4}} \\
      X & \set{w_{1}}
    \end{array}
  \right]
  \]  

\item If we assume that the unknown term has an atomic meaning, then
  we can strengthen the inference (and generate it under a wider range
  of parameters settings).  Under these circumstances, the meaning of
  the known disjunct serves as a \emph{focal point} that the speaker
  and listener can coordinate on for the meaning of the unknown word.
\end{examples}

%=====================================================================

\subsection{Characterization}\label{sec:analysis:characterization}

\begin{examples}
\item The basic characterization is that definitional reading arise
  when disjunction costs are low and $\beta$ is high. Conversely,
  Hurfordian reading arise when disjunction cost are high and $\alpha$
  and $\beta$ are relatively close.
  
\item The intuition: where costs are high, the disjunction has to be
  justified. Letting the two terms overlap reduces the justification,
  whereas exclusivizing provides justification. In other words, the
  apparently undue prolixity of the disjunction (the more general term
  would seem to suffice!) generates an inference that we observe in
  the lexical inferences.
  
\item However, this needs to be qualified by the speaker's desire to
  communicate about the lexicon. If $\beta$ is high, then it might be
  worth paying the disjunction costs for the sake of teaching the
  listener about the lexicon, even if this involves a huge penalty in
  terms of informativity (since definitional readings convey only
  a single term's worth of information).

\item Earlier, we gave this charaterization of the contextual
  requirements for definitional readings. The boldfaced phrases
  connect these ideas with our model.

  \begin{examples}
  \item The speaker is mutually and publicly known to be an expert in
    the relevant domain. \textbf{The speaker observes a lexicon--state
      pair, and the listener seeks to figure out which one, which
      entails moving towards the speaker's lexicon.}      

  \item The speaker is mutually and publicly known to believe the
    listener to be a non-expert in the relevant domain.  \textbf{The
      speaker has lexical uncertainty --- it doesn't assume a lexicon
      but rather tries to infer one.}

  \item The speaker is mutually and publically known to have an
    interest in conveying information about the language itself, even
    if this communicative inefficiency in terms of the information
    conveyed about the world.  \textbf{High $\beta$, low disjunction
      costs.}
  \end{examples}

\item Additionally, the secondary nature of the definitional
  information (as compared with \word{oenophile means wine lover}) is
  captured by the fact that the inference is primarily about the
  lexicon, rather than about the information conveyed --- after all,
  absence the lexical inference, \word{A} would have done the same
  work with lower costs.

\item Hurfordian readings arise in a much wider range of parameters
  settings, that is contexts, because they survive high definitional
  costs are long as they can be justified.

\item Definitional readings exist mainly in the space of low
  disjunction and high $\beta$. This is more rarefied. We believe this
  is reflected in the data: definitional readings are relatively
  infrequent and delicate.

\item Here's a plot that does a good job of conveying the above.  It's
  for a relatively large lexicon: five atomic lexical items and five
  atomic states. (Smaller examples can be hard to interpret, since the
  model's overall pressure to achieve separating equilibria can
  create associations that we wouldn't expect to see in a more complex
  setting like, well, English.) The x-axis is $\log(\beta/\alpha)$, so
  $0.$ marks the points where $\beta = \alpha$. The y-axis is 
  the cost of disjunction (assuming $\gamma=1.0$).

  $\mspace{-80mu}$
  \includegraphics[width=1.2\textwidth]{fig/lex5-alpha-beta-gamma}

\end{examples}

%=====================================================================

\bibliographystyle{apalike}
\bibliography{levy-potts-pragdisj-bib}

\end{document}


