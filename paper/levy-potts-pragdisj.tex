\documentclass{article}

\input{preamble}

\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{Negotiating lexical uncertainty and expertise with disjunctive utterances}
\author{Roger Levy and Christopher Potts}
\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}\label{sec:introduction}

Natural languages are neither fixed across time nor identically
reproduced in all speakers, but rather continually renegotiated during
interactions \citep{Clark97}. Discourse participants accommodate to
each other's usage patterns \citep{Giles:2008,DanLeePan:2012}, form
temporarily lexical pacts to facilitate communication
\citep{Clark:Wilkes-Gibbs:1986}, and instruct each other about their
linguistic views. Some of this communication in language about
language is direct, as with explicit definitions like
\word{`oenophile' means wine lover}, but much of it arrives via
secondary pragmatic inferences, as when \word{Xs and other Ys} conveys
that \word{X} is semantically subsumed by \word{Y} \citep{Hearst92,SnowEtAl05}.

Disjunction supports what appear to be opposing inferences about
language. On the one hand, \word{X or Y} will tend to convey that the
meanings of \word{X} and \word{Y} are presumed to be disjoint
\citep{Hurford:1974}, because the speaker holds such a view of the
lexicon or because she is worried that the listener might. This
pressure to exclusivize is robust enough to overcome even seemingly
non-negotiable aspects of the lexicon, so that ``The nuptials will
take place in either France or Paris'' can be used to suggest the
sense `Paris outside of France' for the first disjunct. In these
cases, disjunction might seem to be a needlessly verbose way of
conveying the meaning of the more general disjunct, but the costs
could be worth paying in virtue of the lexical side-effect of
exclusivization.

On the other hand, disjunctions like \word{wine lover or oenophile}
can be used to convey that the two disjuncts are roughly synonymous
\citep{Horn89,Rohdenburg:1985}, thereby providing secondary
definitional information that maximally violates the pressure to
exclusivize. This inference is more rarefied than the exclusivization
inference, but it can arise in a broad range of contexts in which such
definitional information has social or communicative value. It is
striking that the definitional and exclusivization inferences are
supported by a single lexical item, and the puzzle deepens when we see
that the empirical picture is not a quirk of English, but rather one
that we find in a wide range of typologically and geographically
diverse languages.

In this paper, we capture both of these classes of inference within a
single recursive Bayesian model of pragmatic reasoning. The model
finds its conceptual origins in \posscitet{Lewis69} work on signaling
systems and builds on ideas from iterated best response models
\citet{Jaeger:2007,Jaeger:2011,Franke09DISS} and more thoroughly
probabilistic variants of them
\citep{CamererHo:2004,Frank:Goodman:2012}. The crucial feature of our
model is that it lets discourse participants communicate, not just
about the world, but also about the language they are using
\citep{Bergen:Goodman:Levy:2012,bergen-levy-goodman:2014}; the
speaker's intentions in production are characterized in terms of both
world information and linguistic information, and the listener's
pragmatic reasoning is cast as a problem of joint inference about the
speaker's intended meaning and preferred lexicon
\citep{Smith:Goodman:Frank:2013}. We show that, within this model,
both exclusivization and definitional inferences arise naturally from
the expected semantic content of disjunction, depending on contextual
parameters relating to speaker expertise, listener malleability, and
information contained in the common ground. The model thus offers a
genuinely pragmatic account of these inferences as well as a
characterization of their stability and their communicative value.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Lexical side effects from disjunction}\label{sec:data}

Contextual requirements: 

\begin{examples}
\item The speaker is mutually and publicly known to be an expert in
  the relevant domain.
  
\item The speaker is mutually and publicly known to believe the
  listener to be a non-expert in the relevant domain.
  
\item The speaker is mutually and publically known to have an
  interest in conveying information about the language itself,
  even if this communicative inefficiency in terms of the 
  information conveyed about the world.
\end{examples}

%=====================================================================

\subsection{Hurfordian uses}\label{sec:data:overlapping}

\posscitet{Hurford:1974} generalization (HG) is a direct statement of
the overall communicative pressure to treat disjuncts as exclusive.
It says that, subject to certain exceptions, \word{X or Y} is
felicitous only if the meanings of \word{X} and \word{Y} are disjoint.
\posscitet{Singh:2008} extends this generalization to examples
involving overlapping but non-entailing disjunctions. 

Such disjunctions are more routine in real usage than HG would seem to
suggest. We have collected a large corpus of counterexamples,
available at the following link:
%
\begin{examples}
\item\label{ourcorpus} Counterexamples to Hurford's constraint: \url{http://goo.gl/VAGqnB}
  %
  \begin{examples}
  \item Stop discrimination of an \highlight{applicant or person} due to their tattoos.
  \item Promptly report any \highlight{accident or occurrence}.
  \item The anchor will lie on the bottom and the \highlight{canoe or boat} will be held by the stream's current.
  \item I believe that music can \highlight{change or affect} your emotions.
  \item ``As an \highlight{actor or performer}, you are always worried about what the next job's going to be,'' Hensley says.
  \item Many state arbitration statutes contemplate motions to \highlight{correct or modify} being made to the tribunal directly.
  \item James Clifford (1992, 1997) is the writer most associated with making the connection between culture and \highlight{travel or movement}.
  \item Being a \highlight{captain or officer} is a privilege, and with that privilege comes great responsibility.
  \end{examples}
\end{examples}
%
The dataset includes XXXX cases where the left disjunct entails the
right, and XXXX in which the right entails the left.  In addition, we
conjecture that, for any two nouns $N_{1}$ and $N_{2}$ one believes to
be in an overlap or proper entailment relation, a Web search for
``$N_{1}$ or $N_{2}$'' will generally yield counterexamples, as will
``$N_{2}$ or $N_{1}$''. Ideally, we would develop a rigorous sampling
procedure to try to determine whether the ratio in our corpus reflects
the ratio in usage, but we do not see a way to do this. The primary
obstacle is a deep part of the phenomenon itself: judgments about
lexical entailment are inherently very messy because of the flexible
ways in which we refine meanings in context.

It is in fact an important feature of these data that there is often
considerable uncertainty about whether the two terms are truly in an
entailment relation. For instance, where
\subeg{exclusive}{franceorparis} leaves little room to negotiate the
meaning of the terms in any clear sense,
\subeg{exclusive}{churchorsynagogue} is much less clear: some speakers
have firm judgments that \word{church} and \word{synagogue} exclude
each other, but the many occurrences of ``synagogues and other
churches'' in real text indicate that this is not a universally shared
view of the lexicon.

\begin{examples}
\item\label{exclusive}
  \begin{examples}
  \item\label{franceorparis} ``The nuptials will take place in either
    France or Paris.''
  \item\label{churchorsynagogue} ``In 1940, 37 percent of us had gone
    to a church or synagogue in the last week.''
  \end{examples}
\end{examples}

We should even qualify our assertion that \word{France} and
\word{Paris} invariably stand in an entailment relation. In contexts
where France is being construed in terms of its countryside, or Paris
in terms of its particular urban charms, \word{France} could come to
mean something more like `Paris outside of France'
\citep{Rohdenburg:1985}.

This lexical uncertainty motivates our own explanation for why
speakers utter HG-violating disjunctions (as they clearly do; see
\eg{ourcorpus}).  In broad terms, we say that such examples convey
that the speaker is treating the two terms as exclusive. The
motivations for this could simply be that this is how the speaker
lexicalizes the two terms, which might reflect the listener's lexicon
or clash with it. In such cases, disjunction is the only way for the
discourse participants to use the terms to convey their union.

Some instances are motivated by the speaker seeking to head off an
unwanted inference that the listener might make. For instance,
adopting terms from \citet{Levinson00} and others, we might say that
the speaker is avoiding an I(nformativeness)-implicature: when the
listener might construe the general term as \emph{limited to} a
salient or prototypical subkind. For example, where \word{boat} tends
to identify motorboats, the speaker uses \word{boat or canoe} to
ensure that canoes are included. Alternatively, the speaker might be
heading off a Q(uantity)-implicature: where failure to explicitly
mention a salient or prototypical subkind might trigger an ad-hoc
scalar implicature of its \emph{exclusion}
\citep{hirschberg:1985}. (The speaker uses \word{swimwear or bikini}
to ensure that bikinis are included.)

In both cases, the speaker is concerned that $A$ will be construed as
$\sem{A}-\sem{a}$, so using the disjunction ensures that the overall
denotation of the disjunction is equivalent to $\sem{A}$ even if the
listener no matter what. Anticipating our own explanation, we say that
the speaker is hedging against the possibility that the listener's
view of language is such that, at least in the current context,
$\sem{A} \cap \sem{a} = \emptyset$, or perhaps, more weakly, $ \sem{a}
\nsubseteq \sem{A} = \emptyset$. We think this is well-aligned with
the guiding ideas of \citet{Chemla-HurfordCounts}.

An HG-violating disjunction like \word{boat or canoe} is more prolix
than just \word{boat}, but the meaning of the disjunction includes
\word{canoe} even if the listener interprets \word{boat} with a
narrower refined meaning.  A speaker who places sufficiently high
utility on conveying the broader meaning of \word{boat} will be
willing to pay the cost of the additional disjunct to forestall
misinterpretation.
 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Definitional uses}\label{sec:data:definitional}

Definitional uses of disjunction like \word{wine lover or oenophile}
seem to be rarer and more contextually restricted even than the
somewhat unusual compatible and subsumptive disjunctions reviewed
above. We know they are attested in Chinese, Finnish, German, Hebrew,
Ilokano, Japanese, Russian, and Tagalog, suggesting that, despite
their specialized nature, they represent a principled extension of the
standard uses of disjunction.

The definitional meaning is secondary to the central message --- a
sort of side-effect of the main utterance. This contrasts with overt
definitions like \word{oenophile means `wine-lover'} or
\word{oenophile: wine-lover} (in a dictionary context). Our model
captures this by characterizing the definitional effect as an
inference about the lexicon, rather than about the information
conveyed. Alternative forms achieving the same thing: apposition
\word{oenophile (`wine-lover')}. There might be others. Apposition
would seem to have a conjunctive semantics (broadly speaking), so it
is striking that there seem not to be definitional uses of conjunction
itself.

Definitional readings need not be perfectly definitional in the sense
of complete, pure synonymy. Indeed, it is likely that no definition
achieves this ideal, especially if connotational meaning is
included. (Examples: \word{internet or computer network}, and even our
wine-lover example, where \word{oenophile} seems to elevate the
concept to something more refined than \word{wine lover} picks out.)

Definitional readings arise most easily when the speaker is mutually
and publicly known to be a expert in the domain covered by the terms
and the listener is mutually and publicly known to be inexpert in that
area. In such cases, the speaker can use the disjunction to convey
secondary definitional information. However, other contexts also
support such readings. In some naturalistic uses, definitional
disjunction is also seen when it is not common ground that the
listener doesn't know the novel term, but rather when there is value
for the speaker in demonstrating knowledge of the novel term's
meaning.  We suggest that these uses are derivative of the core usage
that we have presented a model of.

The listener might use a definitional disjunction to convey expertise
indirectly (without asserting it directly) --- an ambitious
undergraduate conveying competence to a professor, for example. In
other cases, the speaker and listener might both be experts in the
domain and see a value (jointly or just in the current speaker's eyes)
of using a word in a specialized sense in order to name a concept
efficiently. In such cases, since the definition is not asserted or
given overtly, but rather inferred, we would expect it to have to be a
relatively normal sort of adjustment. One would not use it to define a
totally novel word, as in \word{an X or `glomp'}, but one might use it
if the intentions are transparent given the new word's shape \word{a
  solar-powered generator or `sologen'} or th eadjustment is minor and
uncontroversial \word{snobish wine lover or `oenophile'}.

For these reasons, we retreat to a more basic characterization: the
discourse participants must have an interest in communicating about
their language and arriving at a refined, perhaps context-specific
joint understanding of it. In addition, we hypothesize that the cost
of using a disjunction must be fairly small all things considered,
else it is hard to see how the speaker could justify using a
disjunction \word{X or Y} to convey simply $\sem{\word{X}}$.


Overt signals: ad hoc prosody, italics, quotation marks. There is
clearly great pressure to signal definitional readings, though it is
not an absolute requirement.


There can be uncertainty about whether the listener should regard the
disjunction as definitional. We might enter into a blury area in which
the listener is struggling to figure out whether the intentions are
definitional or exclusivizing. This can persist even when one of the
words is unknown to the listener. In such cases, the disjunctive
meaning is just extremely general and will not do justice to the
speaker's intentions.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Model}\label{sec:model}

The model finds its conceptual origins in \posscitet{Lewis69} work on
signaling systems and builds on ideas from the iterated best response
models of \citet{Jaeger:2007,Jaeger:2011} and
\citet{Franke09DISS}. Models of this form have been shown to achieve
tight correlations with experimental data (e.g.,
\citealt{Frank:Goodman:2012}) and to offer unified, precise account of
a wide range of conversational implicatures
\citep{bergen-levy-goodman:2014}.

\begin{examples}
\item\label{model}
  \begin{examples}
  \item\label{states}%
    $\States$ is a set of states (worlds, referents, propositions, etc.).
  \item\label{messages}%
    $\Messages$ is a set of messages containing designated `null' message $\nullmsg$.
  \item\label{lex}%
    $\Lex: \Messages \mapsto \wp(\States)$ is a semantic interpretation function. 
    $\Lex'(\nullmsg) = \States$
  \item\label{prior}%
    $\Prior : \States \mapsto [0,1]$ is a prior probability
    distribution over states.    
  \item\label{costs}%
    $\Costs : \Messages \mapsto \Reals$ is a cost function on messages.
    $\Costs(\nullmsg) > \Costs(\msg), {\forall \msg \in \Messages{-}\set{\nullmsg}}$
  \item\label{lexset}%
      $\LexSet = \set{\Lex' : \Lex'(\nullmsg) = \States \text{ and } 
      \forall \msg \in \Messages{-}\set{\nullmsg}, 
      \Lex'(\msg) \neq \emptyset \text{ and } 
      \Lex'(\msg) \subseteq \Lex(\msg)}$
  \item\label{LexPrior}% 
    $\LexPrior : \LexSet \mapsto [0,1]$ is a prior
    probability distribution over lexica.
  \item\label{temps}%
    Real-valued learning-rate parameters $\alpha$, $\beta$, and
    $\gamma$
  \end{examples}
\end{examples}

Clauses~\dashsubeg{model}{states}{costs} define a signaling system. We
imagine that the speaker and listener are playing a game in which the
speaker privately observes a state $\state \in \States$ and produces a
message $\msg \in \Messages$ on that basis, given the context defined
by the signaling system. The listener then uses $\msg$ to guess a
state $\state' \in \States$, and the communication succeeds just in
case $\state = \state'$.

We assume that the set of messages includes a designated null message
$\nullmsg$ that is true in all states in all lexica. It can be thought
of as a catcha-all for the numerous messages in the language that are
not discriminating in the context defined by the signaling system.  No
matter how big and complex our examples grow, we assume that such a
message is always justified. It also helps ensure that various
calculations in the model are well-defined \citep{Jaeger:2011}.

Throughout this basic game, the two players assume that the lexicon is
fixed and mutually, publicly known. In \subeg{model}{lexset}, we see a
first retreat from this assumption. There, we define a set of
refinements of the origianl $\Lex$. In a sense, this defines the space
of pragmatic enrichment allowed by the fixed conventions. Here, we
restrict the lexica to that are refinements of the orginal and assign
non-empty denotations to every message. This is not a deep assumption
of the model, though. One could, for instance, define a much larger
space of lexica and then use the prior $\LexPrior$ to structure
that space with respect to the agents we define just below.

The parameters in \subeg{model}{temps} control the relative weighting
of information about the world, information about the lexicon, and
information about the message costs. We further explicate these
parameters when defining our agents just below.


Our model defines an intuitive hierarchy of agents. The most basic are
$\listenerZero$, $\speakerOne$, and $\listenerOne$. These agents
reason in terms of a fixed lexicon $\Lex$, which, for our purposes,
can be thought of as a standard interpretation function. 

The starting point is $\listenerZero$. This agent simply turns $\Lex$
into a probabilistic formulation that can be used for decision making
in the presence of communucative indeterminacy. The first term just
defines an even distribution over all of the true states $\state$ for
the given message $\msg$, and the second term incorporates the
contextual prior distribution over states $\Prior$.

\begin{examples}
\item\label{l0}%
    $\listenerZero(\state \given \msg, \Lex) \propto
    \frac{\Indicator(\state \in \Lex(\msg))}{|\Lex(\msg)|}
    \Prior(\state)$
\end{examples}

This agent is pragmatic only insofar as it incorporates the contextual 
prior into (a distribution derived from) the truth conditions. Richer
pragmatic inferences start to emerge as soon as we envision a speaker
who can reason in terms of this listener and plan its utterances 
accordingly. The minimal such speaker is $\speakerOne$, which is 
defined recursively in terms of $\listenerZero$:
%
\begin{examples}
  \item\label{s1}% 
    $\speakerOne(\msg \given \state, \Lex) \propto
    \exp
    \left(
      \log\left(\alpha\,\listenerZero(\state \given \msg, \Lex) \right)
      - 
      \gamma\,\Costs(\msg)
    \right)$
\end{examples}
%
Ths definition is a bit cumbersome because of the need to ensure that
all values are positive. At its heart this agent is parallel to
$\listenerZero$ in that it combines a conditional distribution and a
piece of contextual information --- now costs on messages. The result
is a distribution that one can imagaine serving as the basis for
decision making: given that the agent would like to convey that a
certain state holds, which message will do that most effectively for
the listener? The two positive, real-valued parameters $\alpha$ and
$\gamma$ control the rate of learning, in a manner similar to
temperature parameters in reenforcement learning models.

The first pragmatic listener is $\listenerOne$. It is parallel to
$\listenerZero$ except that it reasons, not in terms of the original
lexicon, but rather in terms of $\speakerOne$ reasining about
$\listenerZero$ reasoning about the lexicon:
%
\begin{examples}
  \item\label{l1}% 
    $\listenerOne(\state \given \msg, \Lex) \propto 
    \speakerOne(\msg \given \state, \Lex)
    \Prior(\state)$
\end{examples}

This basic model suffices to derive a range of implicatures involving
context-dependent reasoning about specific terms and their general
counterparts. Broadly speaking, such models can achieve variations on
the theme that using a general term will tend to convey that its
specific counterpart is inappropriate, creating scalar implicatures.
The moadels have been shown to achieve good quantitative fits to
experimental results.  One can also immediately generalize
$\listenerOne$ and $\speakerOne$ to allow them to recursively respond
to each other, which generally has the effect of soldifying
implicatures and predicting new ones that speakers seem less inclined
to perceive \citep{Vogel-etal:2014}.

The generalization we propose is diffent. Rather than defining further
recursion based on these lexicon-specific agents, we now open up the
model to allow for the possibility that the discourse participants are
negotiating not just what information they exchange but also what
language they use to exchange it. Our $\ListenerK$ achieves this on
the listener side; it is based on the social-anxiety listener of
\citet{Smith:Goodman:Frank:2013}.
%
\begin{examples}
  \item\label{Lk}%
    \setlength{\arraycolsep}{2pt}%  
    $\mspace{-4mu}\begin{array}[t]{r c l}
      \ListenerK(\state, \Lex \given \msg) 
      &=&
      \sum_{\Lex \in \LexSet} \listenerOne(\state \given \msg, \Lex) \ListenerK(\Lex \given \msg) 
      \\[1ex]
      \ListenerK(\Lex \given \msg) 
      &\propto& 
      \Prior(\Lex) \sum_{\state\in\States} \SpeakerK(\msg \given \state)\Prior(\state)
    \end{array}$
\end{examples}
%
In terms of its form, the most important feature of this model is that
it makes joint inferences about states and lexica; rather than taking
the lexicon as given, it tries to make guesses about this based on the
speaker's message. The second term models this information
specifically by characterizing the extent to which the speaker's
message discriminates among lexica. This listener is responding to
$\listenerOne$ in sense that it sums over all of the
$\listenerOne(\state\given\msg\Lex)$ inferences for all lexica.
  
Our speaker agent responds to this sophisticated joint listener.  We
assume that the speaker does have a specific lexicon in mind.  More
precisely, this speaker is taken to observe state--lexicon pairs and
produce messages on that basis. In broad terms, it is similar to
$\speakerOne$ except that it includes a new term
$\ListenerK[k-1](\Lex\given\msg)$.

\begin{examples}  
  \item\label{Sk}%
    $\SpeakerK(\msg \given \state, \Lex) \propto 
    \exp
    \left(
      \log
      \left(\alpha\,\ListenerK[k-1](\state \given \msg, \Lex)\right)
      - 
      \beta \log\left(\ListenerK[k-1](\Lex\given\msg)\right)
      -
      \gamma\,\Costs(\msg)
    \right)$
\end{examples}


The above can be unwieldy to think about because of the lexical
inferences. To return to the intuitive picture of simple
state/message signaling, we can sum over lexica, as in 
\eg{lisnorm} and \eg{spknorm}. Both provide useful summaries
of the model's predictions.
%
\begin{examples}
\item\label{lisnorm}% 
  $\ListenerK(\state \given \msg)  = 
  \sum_{\Lex \in \LexSet} \ListenerK(\state, \Lex \given \msg)$
\item\label{spknorm}% 
  $\SpeakerK(\msg \given \state) \propto 
  \sum_{\Lex \in \LexSet} \SpeakerK(\msg \given \state, \Lex)
  \LexPrior(\Lex)$
\end{examples}

\begin{figure}[htp]
  \centering
  \setlength{\tabcolsep}{2pt}
  \begin{tabular}[c]{ *{15}{c} }
   $\listenerZero$ && $\listenerOne$  & $\rightarrow$ & $\ListenerK[1]$ && $\ListenerK[2]$ && $\ListenerK[3]$\\
   \multicolumn{3}{c}{$\searrow$ $\nearrow$} &&\multicolumn{5}{c}{$\searrow$ $\nearrow$ $\searrow$ $\nearrow$ $\searrow$}\\
             & $\speakerOne$ &&&& $\SpeakerK[2]$ && $\SpeakerK[3]$ && $\ldots$   
  \end{tabular}
  \caption{Summary of model structure}
  \label{fig:modstruc}
\end{figure}

%=====================================================================

\section{Scalar implicature of disjunction}\label{sec:scalar-disj}

This initial example is meant to show how the model works and to show
that we can capture the core scalar implicature that disjunction is
best known for.

\begin{examples}
\item 
  \begin{examples}
  \item 
    $\Lex$:
    $\begin{array}[c]{r *{4}{c}}
      \toprule
                    & p      & q      & p \word{ or } q & p \word{ and } q \\
      \midrule
      w_{1}          & \True    & \False & \True     & \False \\
      w_{2}          & \True   & \True  & \True     & \True  \\
      w_{3}          & \False  & \True  & \True     & \False 
      \\[2ex]
      w_{1} \vee w_{2} & \True & \False & \True & \False \\
      w_{1} \vee w_{3} & \False & \False & \True  & \False \\ 
      w_{2} \vee w_{3} & \False & \True & \True  & \False \\
      w_{1} \vee w_{2} \vee w_{3} & \False & \False & \True  & \False 
      \\[2ex]
      w_{1} \wedge w_{2}  & \True  & \True & \True & \True\\
      w_{1} \wedge w_{3}  & \True  & \True & \True & \False\\
      w_{2} \wedge w_{3}  & \True  & \True & \True & \True\\
      w_{1} \wedge w_{2} \wedge w_{3} & \True  & \True & \True & \True\\
      \bottomrule
    \end{array}$
  \item $\alpha = 3$ (it's important that this be high for exclusivization)
  \item Disjunction cost: $0.1$; $\beta = 1$; $\gamma = 1$  
  \end{examples}
\end{examples}

\noindent
\begin{minipage}[c]{0.48\linewidth}
  \begin{examples}
  \item $\ListenerK[3]$ after marginalization 
    
    \vspace{-4pt}

     $\mspace{-90mu}$
     \includegraphics[width=1.2\textwidth]{fig/scalardisj-expertise-listener-marginalized}
  \end{examples}
\end{minipage}
\hfill
\begin{minipage}[c]{0.48\linewidth}
  \begin{examples}
  \item $\SpeakerK[3]$ summed over lexica
    
    \vspace{-4pt}

    $\mspace{-70mu}$
    \includegraphics[width=1.2\textwidth]{fig/scalardisj-expertise-speaker-lexsum}
  \end{examples}
\end{minipage}

\begin{examples}
\item Summary of the above via max associations:

  \begin{minipage}[t]{0.45\linewidth}
    Listener

    \vspace{-2pt}
  
    $\begin{array}[t]{r @{ \ \Rightarrow \ } l}
      \toprule
      \text{Message} & \text{Inference} \\
      \midrule
      p &  w_{1}\\
      q & w_{3} \\
      p \word{ or } q & w_{1} \vee w_{3} \\
      \bottomrule
    \end{array}$
  \end{minipage}
  \hfill
  \begin{minipage}[t]{0.45\linewidth}
    Speaker

    \vspace{-2pt}

    $\begin{array}[t]{r @{ \ \Rightarrow \ } l}
      \toprule
      \text{Observation} & \text{Best message} \\
      \midrule
      w_{1} & p \\
      w_{2} & \set{p, q} \\
      w_{3} & q  \\      
      w_{1} \vee w_{3} & p \word{ or } q \\
      \bottomrule
    \end{array}$
  \end{minipage}    
\end{examples}

%=====================================================================

\section{Analysis}\label{sec:analysis}

\begin{examples}
\item Throughout, let \word{X} be the unknown term.

\item From the listener's perspective, we are concerned to see when
  \word{A or X} gives rise to the inference that $\sem{A} \cap \sem{X}
  = \emptyset$ (Hurfordian reading) and when \word{A or X} gives rise
  to the inference that $\sem{A} = \sem{X}$ (definitional).
\end{examples}

\newcommand{\smallhurfordlex}[3]{
  \left[
    \begin{array}[c]{l@{ \ \mapsto \ }r l@{ \ \mapsto \ }r l@{ \ \mapsto \ }r}
      A & \set{#1} &
      B & \set{#2} &
      X & \set{#3}
    \end{array}
  \right]}

%=====================================================================

\subsection{Subsumptive disjunctions}\label{sec:analysis:subsumptive}

\begin{examples}
\item From the listener's perspective, we are concerned to see when
  \word{A or X} gives rise to the lexical inference that $\sem{A} \cap
  \sem{X} = \emptyset$. We assume that the overall meaning will be
  $\sem{A} \cup \sem{X}$, i.e., that the relevant information concerns
  just the lexical inference.

\item Here's a scenario in which the exclusivization inference arises:

  \begin{examples}
  \item $\States = \set{w_{1}, w_{2}, w_{3}}$;  $\Messages = \set{A, B, X}$
  \item $\Lex = [A \mapsto \set{w_{1}}, B \mapsto \set{w_{2}}, X \mapsto \set{w_{1}, w_{2}}]$
  \item Priors are flat. $\alpha = 2$; $\beta = 1$; $\gamma = 1$. Disjunction cost: $1$. $n = 3$. 
  \end{examples}

\item The listener's best guess inference, upon hearing \word{A or X},
  is that the speaker's state is $w_{1} \vee w_{2}$ and that the
  lexicon is the one where $A$ and $X$ are disjoint. Here's the joint
  probability table:
  \[
  \renewcommand{\arraystretch}{2}
  \begin{array}[c]{l r r r}
    \toprule
            & w_{1} & w_{2} & w_{1} \vee w_{2} \\
    \midrule
    \smallhurfordlex{w_{1}}{w_{2}}{w_{1}, w_{2}} & 0 & 0 & 0.16 \\
    \smallhurfordlex{w_{1}}{w_{2}}{w_{2}} & 0 & 0 & \graycell{0.47} \\
    \smallhurfordlex{w_{1}}{w_{2}}{w_{1}} & 0 & 0 & 0.38 \\
    \bottomrule
  \end{array}
  \]

\item The above parameters deliver the same result with the the
  lexicon size increased (same pattern: each lexical item denotes its
  own world and $X$ denotes the union of all the worlds). This is
  unwieldy to visualize, but the important thing is that the best
  lexicon is always a Hurfordian one. Here's the best lexical
  inference with five atomic messages and four atomic states:
  \[
  \left[
    \begin{array}[c]{l@{ \ \mapsto \ }l}
      A & \set{w_{1}} \\
      B & \set{w_{2}} \\
      C & \set{w_{3}} \\
      D & \set{w_{4}} \\
      X & \set{w_{2}, w_{3}, w_{4}}
    \end{array}
  \right]
  \]
  This is the `minimal' Hurfordian lexicon: the listener doesn't infer
  anything about $X$ except that it is disjoint from $A$.    
\end{examples}

%=====================================================================

\subsection{Definitional disjunctions}\label{sec:analysis:definitional}

\begin{examples}
\item From the listener's perspective, we are concerned to see when
  \word{A or X} is interpreted as equivalent to $\sem{A}$.

\item From the speaker's perspective, we want to know what happens
  when the speaker favors a lexicon in which $\sem{A}=\sem{X}$ and
  observes a state equivalent to the literal meaning of $\sem{A}$.
  When will such a speaker produce \word{A or X}.

\item Here's an example in which these two perspectives are
  complementary. To achieve it, we have to substantially raise
  $\alpha$ and $\beta$ and lower disjunction costs:

  \begin{examples}
  \item $\States = \set{w_{1}, w_{2}, w_{3}}$;  $\Messages = \set{A, B, X}$
  \item $\Lex = [A \mapsto \set{w_{1}}, B \mapsto \set{w_{2}}, X \mapsto \set{w_{1}, w_{2}}]$
  \item Priors are flat. $\alpha = 5$; $\beta = 7$; $\gamma = 1$. Disjunction cost: $0.01$. $n = 3$. 
  \end{examples}

\item The listener's best guess inference, upon hearing \word{A or X},
  is that the speaker's state is $w_{1} \vee w_{2}$ and that the
  lexicon is $\Lex_{1}$. Here's the joint probability table:
  \[
  \renewcommand{\arraystretch}{2}
  \begin{array}[c]{l r r r}
    \toprule
            & w_{1} & w_{2} & w_{1} \vee w_{2} \\
    \midrule
    \smallhurfordlex{w_{1}}{w_{2}}{w_{1}, w_{2}} & 0 & 0 & 0 \\
    \smallhurfordlex{w_{1}}{w_{2}}{w_{2}}        & 0 & 0 & 0 \\
    \smallhurfordlex{w_{1}}{w_{2}}{w_{1}}        & \graycell{0.88} & 0 & 0.12\\
    \bottomrule
  \end{array}
  \]

\item The above parameters deliver the same result with the the
  lexicon size increased. Here's the best lexical inference with five
  atomic messages and four atomic states --- in this way, the 
  listener has learned that $A$ and $X$ are synonymous:
  \[
  \left[
    \begin{array}[c]{l@{ \ \mapsto \ }l}
      A & \set{w_{1}} \\
      B & \set{w_{2}} \\
      C & \set{w_{3}} \\
      D & \set{w_{4}} \\
      X & \set{w_{1}}
    \end{array}
  \right]
  \]  

\item If we assume that the unknown term has an atomic meaning, then
  we can strengthen the inference (and generate it under a wider range
  of parameters settings).  Under these circumstances, the meaning of
  the known disjunct serves as a \emph{focal point} that the speaker
  and listener can coordinate on for the meaning of the unknown word.
\end{examples}

%=====================================================================

\subsection{Characterization}\label{sec:analysis:characterization}

\begin{examples}
\item The basic characterization is that definitional reading arise
  when disjunction costs are low and $\beta$ is high. Conversely,
  Hurfordian reading arise when disjunction cost are high and $\alpha$
  and $\beta$ are relatively close.
  
\item The intuition: where costs are high, the disjunction has to be
  justified. Letting the two terms overlap reduces the justification,
  whereas exclusivizing provides justification. In other words, the
  apparently undue prolixity of the disjunction (the more general term
  would seem to suffice!) generates an inference that we observe in
  the lexical inferences.
  
\item However, this needs to be qualified by the speaker's desire to
  communicate about the lexicon. If $\beta$ is high, then it might be
  worth paying the disjunction costs for the sake of teaching the
  listener about the lexicon, even if this involves a huge penalty in
  terms of informativity (since definitional readings convey only
  a single term's worth of information).

\item Earlier, we gave this charaterization of the contextual
  requirements for definitional readings. The boldfaced phrases
  connect these ideas with our model.

  \begin{examples}
  \item The speaker is mutually and publicly known to be an expert in
    the relevant domain. \textbf{The speaker observes a lexicon--state
      pair, and the listener seeks to figure out which one, which
      entails moving towards the speaker's lexicon.}      

  \item The speaker is mutually and publicly known to believe the
    listener to be a non-expert in the relevant domain.  \textbf{The
      speaker has lexical uncertainty --- it doesn't assume a lexicon
      but rather tries to infer one.}

  \item The speaker is mutually and publically known to have an
    interest in conveying information about the language itself, even
    if this communicative inefficiency in terms of the information
    conveyed about the world.  \textbf{High $\beta$, low disjunction
      costs.}
  \end{examples}

\item Additionally, the secondary nature of the definitional
  information (as compared with \word{oenophile means wine lover}) is
  captured by the fact that the inference is primarily about the
  lexicon, rather than about the information conveyed --- after all,
  absence the lexical inference, \word{A} would have done the same
  work with lower costs.

\item Hurfordian readings arise in a much wider range of parameters
  settings, that is contexts, because they survive high definitional
  costs are long as they can be justified.

\item Definitional readings exist mainly in the space of low
  disjunction and high $\beta$. This is more rarefied. We believe this
  is reflected in the data: definitional readings are relatively
  infrequent and delicate.

\item Here's a plot that does a good job of conveying the above.  It's
  for a relatively large lexicon: five atomic lexical items and five
  atomic states. (Smaller examples can be hard to interpret, since the
  model's overall pressure to achieve separating equilibria can
  create associations that we wouldn't expect to see in a more complex
  setting like, well, English.) The x-axis is $\log(\beta/\alpha)$, so
  $0.$ marks the points where $\beta = \alpha$. The y-axis is 
  the cost of disjunction (assuming $\gamma=1.0$).

  $\mspace{-80mu}$
  \includegraphics[width=1.2\textwidth]{fig/lex5-alpha-beta-gamma}

\end{examples}

%=====================================================================

\bibliographystyle{apalike}
\bibliography{levy-potts-pragdisj-bib}

\end{document}


