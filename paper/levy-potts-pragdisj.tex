\documentclass{article}

\input{preamble}

\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{Negotiating lexical uncertainty and expertise with disjunctive utterances}
\author{Roger Levy and Christopher Potts}
\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Communicating in language about language}\label{sec:introduction}

Natural languages are neither fixed across time nor identically
reproduced in all speakers, but rather continually renegotiated during
interactions \citep{Clark97}. Discourse participants accommodate to
each other's usage patterns \citep{Giles:Coupland:Coupland:1991}, form
temporarily lexical pacts to facilitate communication
\citep{Clark:Wilkes-Gibbs:1986,Brennan:Clark:1996}, and instruct each
other about their linguistic views. Some of this communication in
language about language is direct, as with explicit definitions like
\word{`oenophile' means wine lover}, but much of it arrives via
secondary pragmatic inferences, as when \word{X such as Y} conveys
that \word{X} subsumes \word{Y} \citep{Hearst92,SnowEtAl05}.

Disjunction supports what appear to be opposing inferences about
language. On the one hand, \word{X or Y} tends to convey that the
meanings of \word{X} and \word{Y} are presumed to be disjoint
\citep{Hurford:1974}, because the speaker holds such a view of the
lexicon or is worried that the listener might. This pressure to
exclusivize is robust enough to overcome even seemingly non-negotiable
aspects of the lexicon; a medical webpage warns ``If you still have
symptoms or severe blockage in your arteries, you may need angioplasty
or surgery'', sending a clear signal that angioplasty and surgery
are distinct options. Its continuation presupposes just that: 
``Having one of these procedures may save your leg''. The disjunction
might seem to be a needlessly verbose way of conveying the meaning of
the more general disjunct, but the costs could be worth paying in
virtue of the lexical side-effect of exclusivization.

On the other hand, disjunctions like \word{wine lover or oenophile}
can be used to convey that the two disjuncts are roughly synonymous
\citep{Horn89}, thereby providing secondary information that maximally
violates the pressure to exclusivize. This inference is more rarefied
than the exclusivization inference, but it can arise in a broad range
of contexts in which such definitional or identificational information
has social or communicative value. It is striking that the
definitional and exclusivization inferences are supported by a single
lexical item, and the puzzle deepens when we see that the empirical
picture is not a quirk of English, but rather one that we find in a
wide range of typologically and geographically diverse languages.

In this paper, we capture both of these classes of inference within a
single recursive Bayesian model of pragmatic reasoning. The model
finds its conceptual origins in \posscitet{Lewis69} work on signaling
systems and builds on ideas from iterated best response models
\citep{Jaeger:2007,Jaeger:2011,Franke09DISS} and more thoroughly
probabilistic variants of them
\citep{CamererHo:2004,Frank:Goodman:2012}. The crucial feature of our
model is that it lets discourse participants communicate, not just
about the world, but also about the language they are using
\citep{Bergen:Goodman:Levy:2012,bergen-levy-goodman:2014}: the
speaker's intentions in production are characterized in terms of both
world information and linguistic information, and the listener's
pragmatic reasoning is cast as a problem of joint inference about the
speaker's intended meaning and preferred lexicon
\citep{Smith:Goodman:Frank:2013}. We show that, within this model,
both exclusivization and definitional inferences arise naturally from
the expected semantic content of disjunction, depending on contextual
parameters relating to speaker expertise, listener malleability, and
information contained in the common ground. The model thus offers a
genuinely pragmatic account of these inferences as well as
characterizations of their stability and their communicative value.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Lexical side-effects from disjunction}\label{sec:data}

This section explores the exclusivization and identificational uses of
disjunction. Our goal is to more precisely characterize what the
inferences are like and to begin to characterize which context steer
speakers and listeners toward one or the other. These findings inform
the modeling we describe in \dashsecref{sec:model}{sec:analysis}.

%=====================================================================

\subsection{Hurfordian perceptions and intentions}\label{sec:data:overlapping}

\posscitet{Hurford:1974} generalization (HG) is a direct statement of
the overall communicative pressure to treat disjuncts as exclusive:
%
\begin{quote}
  ``The joining of two sentences by \word{or} is unacceptable if one
  sentence entails the other; otherwise the use of \word{or} is
  acceptable.'' (p.~410)
\end{quote}
%
The generalization is stated in terms of sentences, but Hurford's
examples, given in \eg{hex} with his original judgments, make it clear
that he intends it to hold for sub-sentential disjuncts as well (he is
likely assuming conjunction reduction):
%
\begin{examples}
\item\label{hex}
  \begin{examples}
  \item Ivan is an American or Russian.
  \item The painting is of a man or a woman.
  \item The value of $x$ is greater than or equal to 6.
  \item\label{ex-bad1}\bad John is an American or Californian.
  \item\label{ex-bad2}\bad The painting is of a man or a bachelor.
  \item\label{ex-bad3}\bad The value of $x$ is greater than or not equal to 6.
  \end{examples}
\end{examples}

\citeauthor{Hurford:1974} uses HG to probe the nature and distribution
of conversational implicatures (see also
\citealt{Gazdar79b,ChierchiaFoxSpector08}), and \citet{Singh:2008}
extends it to cases in which the disjuncts are merely overlapping. We
endorse the guiding insight behind these accounts but reject the
assumption that HG violations reliably lead to, or even correlate
with, unacceptability or ungrammaticality. Disjunctions of apparently
entailing phrases are routine; all of the examples marked as
ungrammatical in \eg{hex} are easy to find in fluent English text on
the Web:
%
\begin{examples}
\item\label{hex-good}
  \begin{examples}
  %\item ``How much does the average american or californian pay every
  %  year toward state taxes?''
  \item ``\ldots and we trust that some of our \highlight{American or Californian}
    friends will tell us something of its growth of flower and fruit
    in its native habitats''
  \item ``It doesn't matter if you ask \highlight{a boy or a man or a bachelor or
    even a husband}, \ldots''
  \item ``the effect was \highlight{greater than, or not equal to,} the cause.''
  \end{examples}
\end{examples}

We have collected a large corpus of apparent counterexamples,
available at \url{http://goo.gl/VAGqnB}. Here is a small sample from
that corpus:
%
\begin{examples}
\item\label{ourcorpus} 
  %
  \begin{examples}
    %%%%%%%%%% X < Y
  \item Stop discrimination of an \highlight{applicant or person} due
    to their tattoos.
  \item Promptly report any \highlight{accident or occurrence}.
  \item The anchor will lie on the bottom and the \highlight{canoe or
      boat} will be held by the stream's current.
  \item ``As an \highlight{actor or performer}, you are always worried
    about what the next job's going to be,'' Hensley says.
    %%%%%%%%%%  X > Y
  \item After the loss of the \highlight{animal or pet}, there are
    further coping strategies available for the grieving individual.
  \item Bush was captured slyly removing \highlight{a candy or gum}
    from his mouth.
  \item Heroic is not a word one uses often without embarrassment to
    describe a \highlight{writer or playwright} \ldots
  \item But he never attended school during his senior year, never
    attended a \highlight{party or prom}.
  \end{examples}
\end{examples}
%
The dataset includes XXXX cases where the left disjunct entails the
right, and XXXX in which the right entails the left.  In addition, we
conjecture that, for any two nouns $N_{1}$ and $N_{2}$ one believes to
be in an overlap or proper entailment relation, it will generally be
possible to find a context in which ``$N_{1}$ or $N_{2}$'' and
``$N_{2}$ or $N_{1}$'' are felicitous, and Web searches will generally
yield examples.

Eager for a more rigorous assessment of the prevalence of HG-violating
disjunctions, we conducted a simple experiment. First, we obtained a
list of countries and their capitals and, to keep the search procedure
simple, extracted the subset in which the country and capital both
have single word names. Second, we search for `\emph{country or
  capital}' and `\emph{capital or country}' in the Google Books
N-grams data, restricting attention to books after 1960 to try to
avoid some of the encoding difficulties that plague earlier texts in
that corpus. The results of this experiment are summarized in
\tabref{fig:capitals} (the raw data are available at the website
for this paper).

\begin{figure}[tp]
  \centering
  \begin{subtable}{0.45\textwidth}
    \begin{tabular}[c]{r r r}    
      \toprule
      & Attested  \\
      \midrule
      \emph{country or capital}  & N/TOTAL \\
      \emph{capital or country}  & N/TOTAL \\
      \bottomrule
    \end{tabular}
    \caption{Percentage attested limiting to cases where both
      \emph{country} and \emph{capital} are found in the data
      separately.}
  \end{subtable}
  \hfill
  \begin{subtable}{0.45\textwidth}
    \begin{tabular}[c]{l}
      Barplot of \emph{country or capital} \\
      and \emph{capital or country}\\
      Summary based on t-test or similar.
    \end{tabular}
    \caption{Relative frequencies of the two orders. A Wilcoxon
      signed-rank test XXXX.}      
  \end{subtable}
  \caption{Countries and their capitals in disjunctions.}
  \label{fig:capitals}
\end{figure}

Of course, one would like to extend the above experiment to a broader
and more diverse sample of the lexicon, to get a clearer picture of
the prevalance of these disjunctions and to probe ordering preferences
that might relate to the direction of entailment. However, we do not
see a way to do this accurately. The primary obstacle is, we believe,
an important property of the phenomenon itself: judgments about
lexical entailment are inherently very messy because of the flexible
ways in which people refine meanings in context. As a result, there
often isn't a single objective answer to the question of whether two
disjuncts stand in an an entailment relation. For instance, whereas
\subeg{exclusive}{franceorparis} leaves little room to negotiate the
meaning of the terms in any clear sense,
\subeg{exclusive}{churchorsynagogue} is much less clear.
%
\begin{examples}
\item\label{exclusive}
  \begin{examples}
  \item\label{franceorparis} ``The nuptials will take place in either
    \highlight{France or Paris}.''
  \item\label{churchorsynagogue} ``In 1940, 37 percent of us had gone
    to a \highlight{church or synagogue} in the last week.''
  \end{examples}
\end{examples}
%
Some speakers have firm judgments that \word{church} and
\word{synagogue} exclude each other, making this case clearly
HG-respecting. However, it is easy to find uses of the phrase
``synagogues and other churches'', which presuppose that a synagogue
is a kind of church. And we should take care even with our assertion
that \word{France} and \word{Paris} invariably stand in an entailment
relation. In contexts where France is being construed in terms of its
countryside, or Paris in terms of its particular urban charms,
\word{France} could come to mean something more like `Paris outside of
France'. The important thing for our purposes is that the insight
behind HG shines through this uncertainty: no matter what one's
initial view of the lexicon is, disjunctions like \word{X or Y} make
salient a construal of the lexicon in which the disjuncts are
semantically disjoint. The speaker will be perceived as endorsing such
a view, at least for the current conversational exchange, and the
listener can either adopt that assumption or push back.

This lexical uncertainty motivates our own explanation for why
speakers utter HG-violating disjunctions. In broad terms, we say that
such examples convey that the speaker is treating the two terms as
exclusive. There are many potential motivations for this. Perhaps the
most mundane is that the speaker simply lexicalizes the two terms as
exclusive. The disjunction is likely to be easily justified in such
cases, as it might be the most efficient and direct way of identifying
the union of the two terms.

More interesting are cases in which the speaker's disjunction seems to
be part of an attempt to manage the listener's inferences. For
instance, the speaker who uses the phrase \word{swimwear or bikini}
might be concerned that using \word{swimwear} alone will trigger an
ad-hoc scalar implicature \citep{Hirschberg85} excluding the salient
subkind of bikinis. \citet{Chemla-HurfordCounts} studies this class of
inferences, presenting suggestive evidence that the frequency of
disjunctions \word{X or Y} (\word{X} subsuming \word{Y}) is positively
correlated with the likelihood that \word{X} conversationally
implicates \word{not Y} as estimated by the experimental results of
\citet{vanTiel-etal:2013}.  \citeauthor{Chemla-HurfordCounts}'s
experiment relies on the page counts in Google search results, which
are notoriously unreliable \citep{Liberman:2005}, so we reproduced his
main finding using Google Books data \citep{Michel-etal:2011} and a
more direct method for assessing the relative frequency of \word{X or
  Y}. \Figref{fig:chemla} summarizes this experiment, and the raw data
are available at the website associated with this paper.

\begin{figure}[tp]
  \centering
  \includegraphics[width=1\textwidth]{fig/chemla}
  \caption{A corpus study inspired by \citet{Chemla-HurfordCounts} on
    the relationship between the probability that \word{X}
    conversationally implicates \word{not-Y} and the 
    frequency of disjunctions \word{X or Y} relative to 
    the more general term \word{X}. A linear regression (blue line)
    suggests a modest but robust correlation (coef = $6.580^{-06}$; 
    standard error $3.095^{-06}$; $p = 0.04$; $R^{2} = 0.08$).}
  \label{fig:chemla}
\end{figure}

The speaker might wish to block a subtly different kind of implicature
in a situation in which a general term is prone to being limited to a
prototypical subkind \citep{Levinson00}. For instance, at a busy
marina in water-skiing country, \word{boat} might come to identify
just motorboats. In that case, a speaker might use \word{boat or
  canoe} or \word{boat or kayak} to ensure that these non-motorized
cases are included in a rule, lest people assume that these rarer
kinds of boat are exempt.

In both of these implicature-blocking scenarios, the speaker is
concerned that the general term \word{X} will be construed as
$\sem{\word{X}}-\sem{\word{Y}}$ for some \word{Y}. Thus, \word{X} or
\word{Y} is a hedge against the possibility that the listener's view
of language is such that, at least in the current context,
$\sem{\word{X}} \cap \sem{\word{Y}} = \emptyset$, or, as in
\posscitet{Singh:2008} account,
$\sem{\word{Y}} \nsubseteq \sem{\word{X}}$. This is a defensive
position; the speaker's own lexicon might allow her to use just the
general term to convey her intentions, but she is concerned that the
listener will arrive at a different conclusion. The costs of
disjunction are therefore worth paying even if it adds no new
information given the speaker's lexicon.  However, the speaker can
play a more active role as well, using disjunctions to instruct the
listener about the right lexicon. Our \secref{sec:introduction}
example containing ``you may need angioplasty or surgery'' seems to be
an instance of this: the disjunction conveys secondary information
that \word{angioplasty} and \word{surgery} will be treated as separate
options in the current discourse. If HG were adopted as an explicit
theoretical constraint, then the possibility of doing this would more
or less follow --- we just require the additional premise that the
listener is charitable and so will try to find an acceptable construal
of the utterance. The model we develop in \secref{sec:model} also
supports this kind of reasoning, but it requires no independent
statement of HG.
 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Definition and identification}\label{sec:data:definitional}

% A Geological History of Manhattan or New York Island

Disjunctions like \word{wine lover or oenophile} seem to fly in the
face of the Hurfordian pressure reviewed just above. Rather than
avoiding overlap, they seem to embrace it, conveying something
approximating identity. These readings seem to be more contextually
restricted even than HG-violating disjunctions, and speakers often
(but not always) signal them with ad hoc prosody, italics, quotation
marks, and other devices. However, it would be a mistake to dismiss
them as an idiosyncracy of English, since these uses are widely
attested in typologically diverse languages (we have examples from
Chinese, German, Hebrew, Ilokano, Japanese, Russian, and
Tagalog). Even languages that seem to have a dedicated `definitional'
or `metalinguistic \word{or}' (e.g., Finnish, Italian) seem also to
allow the regular \word{or} to play this role.

For our purposes, the most important property of these uses is that
they convey a meaning that is secondary to the main content of the
utterance --- an extreme instance of a meaning that is not at issue
\citep{Tonhauser-etal:2011}. This contrasts with overt definitions
like \word{oenophile means wine-lover} or \word{oenophile: wine-lover}
(in a dictionary context).  We think it is no accident that another
strategy for conveying definitional information in this non-asserted,
taken-for-granted manner is via apposition, as in \word{oenophile
  (`wine-lover')} \citep{Potts05BOOK}. In this respect, the relevant
inference resembles the exclusivization pressure identified by HG:
both seem to emerge as side-effects rather than normal outputs. In our
model (\secref{sec:model}), both are in turn characterized as
inferences about the lexicon rather than about the state of the world.

In addition, as with disjunct exclusivization, the relevant lexical
inference might be temporary. For instance, phrases like
\word{Internet or computer network} seem to use the second phrase as a
rough-and-ready way of helping the listener bootstrap towards an
understanding of what the Internet is like. Even our wine-lover
example seems to involve only approximate synonymy; \word{wine lover
  or oenophile} seems apt in a context in which the speaker wishes to
use \word{oenophile} to elevate the concept to something more specific
(or pretentious) than \word{wine lover} picks out. Similarly, the book
title \emph{A Geological History of Manhattan or New York Island}
seems to identify Manhattan and New York Island while at the same time
acknowdging the different histories and connotations of the two
disjunct terms.

The speaker's motivations for using definitional disjunction are quite
varied. Such readings seem to arise most easily when the speaker is
mutually and publicly known to be a expert in the domain covered by
the terms and the listener is mutually and publicly known to be
inexpert in that area. In such cases, the speaker can use the
disjunction to convey information about her preferred lexicon, fairly
certain that the listener will be receptive. However, while speaker
expertise seems to be a genuine prerequisite, the listener's knowledge
seems to impose much less.  We find natural uses of this strategy when
there is no direct information about the listener, but rather just a
general assumption that one of the terms is relatively unknown. For
instance, a newspaper article might contain \word{wine lover or
  oenophile} without presuming that all its readers are ignorant;
rather, such a use would seem to presuppose only that \word{oenophile}
is relatively unknown.  Finally, at the the other end of the extreme,
the listener might actually be presumed to know the term, but the
speaker sees social value in conveying that she shares this view. This
could be because the speaker would like to display expertise, as when
an ambitious undergraduate seeks to convey competence to a professor.
The uses also arise when the speaker and listener might both be
experts in the domain and see a value (jointly or just in the current
speaker's eyes) of using a word in a specialized sense in order to
name a concept efficiently (e.g., \word{What motivates the snobbish
  wine lover or `oenophile' and how does he differ from the casual
  drinker?}).

For these reasons, we retreat to a more basic characterization: the
discourse participants must have a mutual interest in communicating
about their language and arriving at a refined, perhaps
context-specific joint understanding of it, and there should be a
background assumption that speaker and listener are willing to
coordinate on the lexicon that the speaker seems to be using. In
addition, we hypothesize that the cost of using a disjunction must be
fairly small, all things considered, else it is hard to see how the
speaker could justify using a disjunction \word{X or Y} to convey
simply $\sem{\word{X}}$. At any rate, whatever the costs of using the
verbose form, they must be worth paying in virtue of the benefits of
identifying (for the purposes of the current talk exchange) the
meanings of the two disjuncts.

% There can be uncertainty about whether the listener should regard
% the disjunction as definitional. We might enter into a blurry area
% in which the listener is struggling to figure out whether the
% intentions are definitional or exclusivizing. This can persist even
% when one of the words is unknown to the listener. In such cases, the
% disjunctive meaning is just extremely general and will not do
% justice to the speaker's intentions.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Modeling communication with anxious experts}\label{sec:model}

We now present our model of pragmatic reasoning. Our presentation is
somewhat compact. Readers wishing to more fully explore the model are
referred to the website for this paper, which implements the model,
provides simple and complex examples, and includes (Python) code for
all the examples and figures in this paper.

In our model, production and interpretation are based in a recursive
process in which speaker and listener agents reason about each other
reasoning about each other.  At the lowest levels of our model, these
agents communicate using a single lexicon. However, we do not actually
assume that a single lexicon is mutually and publicly recognized as
the set of core conventions of the language. Rather, our model
aggregates over many possible lexica, thereby allowing the agents to
negotiate the structure of the language itself even as they
communicate. \Figref{fig:modstruc} summarizes this picture
schematically; this section is devoted to explicating these agents and
their relationships.

\begin{figure}[tp]
  \centering
  \newcommand{\labelednode}[2]{\put(#1){\makebox(0,0){#2}}}
  \newcommand{\picarrow}[3][0.9]{\put(#2){\vector(#3){#1}}}
  \newcommand{\picdownarrow}[1]{\picarrow{#1,0.8}{3,-2}}
  \newcommand{\picuparrow}[1]{\picarrow{#1,0.2}{3,2}}
  \setlength{\unitlength}{1cm}
  \begin{picture}(8,1.5)
    \labelednode{1,1.5}{\textbf{Fixed $\Lex$}}
    \labelednode{5.5,1.5}{\textbf{Reasoning about possible $\Lex$}}
    \labelednode{0,1}{$\listenerZero$}
    \picdownarrow{0}
    \labelednode{1,0}{$\speakerOne$}
    \picuparrow{1}
    \labelednode{2,1}{$\listenerOne$}
    \picarrow[0.7]{2.1,1}{1,0}
    \labelednode{3,1}{$\ListenerK[1]$}
    \picdownarrow{3}
    \labelednode{4,0}{$\SpeakerK[2]$}
    \picuparrow{4}
    \labelednode{5,1}{$\ListenerK[2]$}
    \picdownarrow{5}
    \labelednode{6,0}{$\SpeakerK[3]$}
    \picuparrow{6}
    \labelednode{7,1}{$\ListenerK[3]$} 
    \picdownarrow{7}
    \labelednode{8,0}{$\ldots$}
  \end{picture}
  \caption{Summary of model structure.}
  \label{fig:modstruc}
\end{figure}

The core structures of our model are given in \eg{model}.
Intuitively, we imagine that a speaker and listener are playing a game
in which the speaker privately observes a state $\state \in \States$
and produces a message $\msg \in \Messages$ on that basis, given the
context defined by the signaling system. The listener then uses $\msg$
to guess a state $\state' \in \States$, and the communication succeeds
just in case $\state = \state'$. The agents that we define are
rational in the sense that, by reasoning recursively about each
other's behaviors, they can increase their chances of success at this
signaling game.
%
\begin{examples}
\item\label{model}
  \begin{examples}
  \item\label{states}%
    $\States$ is a set of states (worlds, referents, propositions, etc.).
  \item\label{messages}%
    $\Messages$ is a set of messages containing designated `null' message $\nullmsg$.
  \item\label{lex}%
    $\Lex: \Messages \mapsto \wp(\States)$ is a semantic interpretation function. 
    $\Lex'(\nullmsg) = \States$.
  \item\label{prior}%
    $\Prior : \States \mapsto [0,1]$ is a prior probability
    distribution over states.    
  \item\label{costs}%
    $\Costs : \Messages \mapsto \Reals$ is a cost function on messages.
    $\Costs(\nullmsg) > \Costs(\msg), {\forall \msg \in \Messages{-}\set{\nullmsg}}$.
  \end{examples}
\end{examples}

Clause~\subeg{model}{messages} designates a null message $\nullmsg$
that is true in all states in all lexica. It can be thought of as a
catch-all for the numerous messages in the language that are not
discriminating in the context defined by the signaling system.  No
matter how big and complex the examples, such a message is always
justified. Introducing $\nullmsg$ also helps ensure that various
calculations in the model are well-defined. (For alternative methods
of ensuring definedness, see \citealt{Jaeger:2011} and
\citealt{bergen-levy-goodman:2014}.)

%=====================================================================

\subsection{Simple state/message signaling}\label{sec:rsa}

As \figref{fig:modstruc} shows, our model defines an intuitive hierarchy
of agents. The most basic are $\listenerZero$, $\speakerOne$, and
$\listenerOne$. These agents reason in terms of a fixed lexicon
$\Lex$, which, for our purposes, can be thought of as a standard
semantic interpretation function, as in \subeg{model}{lex}. These
agents suffice to define a version of the Rational Speech Acts model
of \citet{Frank:Goodman:2012} and \citet{Goodman:Stuhlmuller:2013}.

The starting point is $\listenerZero$. This agent simply turns $\Lex$
into a probabilistic formulation that can be used for decision-making
in the presence of communicative indeterminacy. The first term just
defines an even distribution over all of the true states $\state$ for
the given message $\msg$, and the second term incorporates the
contextual prior distribution over states $\Prior$.
%
\begin{examples}
\item\label{l0}%
  $\listenerZero(\state \given \msg, \Lex) \propto
  \frac{\Indicator(\state \in \Lex(\msg))}{|\Lex(\msg)|}
  \Prior(\state)$
\end{examples}

This agent is pragmatic only insofar as it incorporates the contextual 
prior into (a distribution derived from) the truth conditions. Richer
pragmatic inferences start to emerge as soon as we envision a speaker
who can reason in terms of this listener and plan its utterances 
accordingly. The minimal such speaker is $\speakerOne$, which is 
defined recursively in terms of $\listenerZero$:
%
\begin{examples}
\item\label{s1}% 
  $\speakerOne(\msg \given \state, \Lex) \propto
  \exp
  \left(
    \log\left(\alpha\,\listenerZero(\state \given \msg, \Lex) \right)
    - 
    \gamma\,\Costs(\msg)
  \right)$
\end{examples}
%
Ths definition is a bit cumbersome because of the need to ensure that
all values are positive. At its heart, though, this agent is parallel
to $\listenerZero$ in that it just combines a conditional distribution
and a piece of contextual information --- now costs on messages. The
result is a distribution that one can imagaine serving as the basis
for decision making: given that the agent would like to convey that a
certain state holds, which message will do that most effectively for
the $\listenerZero$ listener? The real-valued parameter $\alpha$
controls the degree to which the speaker tries to capitalize on the
distinctions encoded by $\listenerZero$.

The first pragmatic listener is $\listenerOne$. It is parallel to
$\listenerZero$ except that it reasons, not in terms of the original
lexicon, but rather in terms of $\speakerOne$ reasoning about
$\listenerZero$ reasoning about the lexicon. This gives rise to a
derive, pragmatically-enriched probabilistic interpretation function.
%
\begin{examples}
\item\label{l1}% 
  $\listenerOne(\state \given \msg, \Lex) \propto 
  \speakerOne(\msg \given \state, \Lex)
  \Prior(\state)$
\end{examples}

\Figref{fig:rsa-disj} shows how this model derives a basic scalar
implicature. We've used disjunction to illustrate, but the reasoning
is actually general across all instances in which a general term $X$
competes with a specific subterm $Y$. In terms of the communication
game, suppose the speaker's private state is $w_{2}$. The literal
listener $\listenerZero$ has only a 50/50 chance of identifying this
state given the message $\porq$.  However, the first pragmatic
listener $\listenerOne$, reasoning in terms of $\speakerOne$, has
already learned to separate $\porq$ and $\pandq$. The nature of
$\speakerOne$ is equally interesting, as it has achieved a
production-oriented version of the scalar implicature, as a bias for
producing $\pandq$ where it is true.

This basic model has been shown to achieve good quantitative fits to
experimental results
\citep{Degen:Franke:2012,Stiller:Goodman:Frank:2011}. One can also
generalize $\listenerOne$ and $\speakerOne$ to allow them to
recursively respond to each other. This has the effect of soldifying
implicatures, but it seems also to predict complex ones that speakers
are less inclined to perceive \citep{Vogel-etal:2014}.

\begin{figure}[tp]
  \centering
  \setlength{\arraycolsep}{3pt} 
  $\begin{array}[b]{c c r@{\hspace{25pt}}l}
     \listenerMatrix%
     {\Lex_{1}}%
     {\True & \True  & \False}
     {\True & \False & \False}     
     {\True & \True  & \True}      
     &
     \rightarrow
     &
     \listenerMatrix%
     {\listenerZero}%
     { .5  &  .5 & 0}%
     {  1 &   0 &  0}%
     {.33 & .33 & .33}%
     &
     \listenerMatrix%
     {\listenerOne}%
     {.25 & .75 & 0}%
     {1 &   0 & 0}%
     { 0 &   0 & 1}
   \\
   && \searrow & \nearrow
   \\
   &&
   \multicolumn{2}{c}{
      \speakerMatrix%
      {\speakerOne}%
      {.33 & .67 & 0}%
      {1 &   0 & 0}%
      {0 &   0 & 1}
   }
   \end{array}$   
   \caption{Illustrating simple state/message signaling with
     disjunction.  We assume an even prior over states and $0$ costs
     for all messages. The recursive process separates disjunction
     (general) and conjunction (specific), thereby creating a pattern
     characterizable as a scalar implicature.}
  \label{fig:rsa-disj}
\end{figure}

%=====================================================================

\subsection{Lexical uncertainty and expertise}\label{sec:full}

In the above simple mode, the agents condition on (take as given) a
fixed, shared lexicon. Howver, the data in \secref{sec:data} show that
the lexicon is not known precisely, but rather negotiated during
interactions. We now bring that insight into our model. The first step
is to define a space of lexica $\LexSet$ and a probability
distribution over them $\LexPrior$:
%
\begin{examples}
\item\label{model-extend}
  \begin{examples} 
  \item\label{lexset}%
      $\LexSet = \set{\Lex' : \Lex'(\nullmsg) = \States \text{ and } 
      \forall \msg \in \Messages{-}\set{\nullmsg}, 
      \Lex'(\msg) \neq \emptyset \text{ and } 
      \Lex'(\msg) \subseteq \Lex(\msg)}$
  \item\label{LexPrior}% 
    $\LexPrior : \LexSet \mapsto [0,1]$ is a prior
    probability distribution over lexica.  
  \end{examples}
\end{examples}
%
Clause~\subeg{model-extend}{lexset} defines a complete set of
refinements of a basic lexicon: every lexical item can denote a subset
of the space it denotes in the focal lexicon $\Lex$, and all
combinations of these refinements are available. The only requirements
are that messages always have non-empty denotations and that the null
message $\nullmsg$ always denotes the full set of states. The
resulting definition gives us an intuitive, computationally manageable
space to work with. We should emphasize, though, that these
assumptions are not essential to our model. In the extreme case, we
could admit every lexicon derivable from the messages $\Messages$ and
states $\States$, and then usee the lexicon prior $\LexPrior$ to
provide some structure to the space --- say, but assigning low but
non-zero probability to lexica that are not refinements. This is a
strictly more general perspective than the one given by
\eg{model-extend}, which can alternatively be seen assigning zero
probability to lexica not admitted by
clause~\subeg{model-extend}{lexset}. We do not explore these options
further in this paper, but see for discussion of non-refinement
pragmatic enrichment.

\Figref{fig:disjlexica} shows the set of admissible lexica for the
initial lexicon $\Lex$ of \figref{fig:rsa-disj}. Since $\pandq$
already denotes a singleton, it admits no refinements, so its meaning
is the same in all three lexica. The null message $\nullmsg$ is fixed
by stipulation. The message of interest is disjunction, since it has
two refinements in addition to its basic meaning. Even in this simple
example, one can see how useful biases will start to emerge. For
instance, $\porq$ has two lexica in which its meaning includes
$w_{2}$, but just one in which it includes $w_{1}$. This bias is
already pushing in the direction of the target pragmatic association
of $\porq$ with $w_{2}$ that we see for $\listenerOne$ in
\figref{fig:rsa-disj}.

\begin{figure}[tp]
  \centering
  \setlength{\arraycolsep}{1pt}    
  $\begin{array}[c]{c c c}
     \Lex & \Lex_{2} & \Lex_{3} \\
     \disjlexicon{w_{1}, w_{2}}{w_{1}}
     & 
     \disjlexicon{w_{1}}{w_{1}}
     & 
     \disjlexicon{w_{2}}{w_{1}}
  \end{array}$
  \caption{Lexicon set $\LexSet$ for the basic lexicon $\Lex$ in \figref{fig:rsa-disj}.}
  \label{fig:disjlexica}
\end{figure}

Our first `lexical uncertainty' agent is $\ListenerK[1]$, which is based
on the model of \citet{Smith:Goodman:Frank:2013}:
%
\begin{examples}
  \item\label{L1}%
    \setlength{\arraycolsep}{2pt}%  
    $\mspace{-4mu}\begin{array}[t]{r c l}
      \ListenerK(\state, \Lex \given \msg) 
      &=&
      \ListenerK(\state \given \msg, \Lex) \ListenerK(\Lex \given \msg) 
      \\[1ex]
      \ListenerK(\Lex \given \msg) 
      &\propto& 
      \Prior(\Lex) \sum_{\state\in\States} \SpeakerK(\msg \given \state, \Lex)\Prior(\state)
    \end{array}$
\end{examples}
%
This agent is defined for levels $k \geqslant 1$ as long as we assume
that $\SpeakerK[1] = \speakerOne$ (higher levels use the speaker in
\eg{Sk}) and
$\ListenerK[1](\state \given \msg, \Lex) = \listenerOne(\state
\given \msg, \Lex)$.
The agent uses the speaker's message to make a joint inference about
the state and the lexicon. The first term uses a fixed-lexicon
inference and the second encodes the extent to which the current
message biases in favor of $\Lex$.

Our `expertise' speaker responds to this lexical uncertainty listener.
We assume that this speaker does have a specific lexicon in mind.  More
precisely, this speaker is taken to observe state--lexicon pairs and
produce messages on that basis. It is defined for all $k > 1$:

\begin{examples}  
  \item\label{Sk}%
    $\SpeakerK(\msg \given \state, \Lex) \propto 
    \exp
    \left(
      \log
      \left(\alpha\,\ListenerK[k-1](\state \given \msg, \Lex)\right)
      - 
      \beta \log\left(\ListenerK[k-1](\Lex\given\msg)\right)
      -
      \gamma\,\Costs(\msg)
    \right)$
\end{examples}
%
In broad terms, this agent is similar to $\speakerOne$ except that it
includes a new term $\ListenerK[k-1](\Lex\given\msg)$ encoding the
information each message conveys about the lexicon. The relative
importance of this information is controlled by a new real-valued
learning-rate parameter $\beta$. The relative weights of $\alpha$ and
$\beta$ control the relative importance of conveying information about
the world and information about the lexicon, and much of our
investigation of disjunction focuses on this relationship.

\Figref{fig:full-disj} shows this model in action on our simple
disjunctive scalar implicature. The left column gives the
$\listenerOne$ inferences for the three lexica in
\figref{fig:disjlexica}. The middle column gives the uncertainty
listener's joint probabilities given each message. It says, for
example that the listener's best-guess (highest probability) inference
given the message $\porq$ is state $w_{2}$ and lexicon $\Lex_{1}$ or
$\Lex_{3}$. The right column shows the first expertise speaker's
message probabilities given observed state--lexicon pairs. From here,
the iteration between $\ListenerK[k-1]$ and $\SpeakerK$ could continue.

\begin{figure}[tp]
  \centering
  $\begin{array}{c c c}
     \listenerOne & \ListenerK[1] & \SpeakerK[2] 
     \\[1ex]     
     \listenerMatrix%
     {\Lex_{1}}%
     {.25 & .75 & 0}%
     {1 & 0 & 0}%
     {0 & 0 & 1}     
     &
     \ListenerKMatrix%
     {\porq}%
     {.12 & .35 & 0}%
     {.18 & 0 & 0}
     {0 & .35 & 0}            
     &
     \SpeakerKMatrix%
     {1}%
     {.31 & .69 & 0}%
     {1 & 0 & 0}%
     {0 & 0 & 1}
     %
     \\[8ex]
     %
     \listenerMatrix%
     {\Lex_{2}}%
     {1 & 0 & 0}%
     {1 & 0 & 0}%
     {0 & .5 & .5}%
     &
     \ListenerKMatrix%
     {\pandq}%
     {.31 & 0 & 0}
     {.23 & 0 & 0}
     {.46 & 0 & 0}
     &
     \SpeakerKMatrix%
     {2}%
     {.18 & .82 & 0}%
     {.99 & 0 & .01}%
     {0 & 0 & 1}%       
     %
     \\[8ex]
     %
     \listenerMatrix%
     {\Lex_{3}}%
     {0 & 1 & 0}%
     {1 & 0 & 0}%
     {0 & 0 & 1}%
     &
     \ListenerKMatrix%
     {\nullmsg}%
     {0 & 0 & .25}%
     {0 & .25 & .25}%  
     {0 & 0 & .25}%
     &
     \SpeakerKMatrix%
     {3}%
     {.18 & .82 & 0}%
     {1 & 0 & 0}%
     {0 & 0 & 1}%  
   \end{array}$
   \caption{Full model inferences for disjunction. $\alpha = 1$
   $\beta = 1$. $\Costs(\nullmsg) = 5$.}
  \label{fig:full-disj}
\end{figure}

%=====================================================================

\subsection{A return to simple signaling}\label{sec:return}

The full model can be unwieldy to think about because of the lexicon
inferences. To return to the intuitive picture of simple state/message
signaling, we can sum over lexica, as in \eg{lisnorm} and
\eg{spknorm}. Both provide useful summaries of the model's
predictions. \Figref{fig:simple} by using these equations to reduce
the middle and right columns of \figref{fig:full-disj} to just 
state/message relationships. The results closely match those obtained
from the simpler system (\figref{fig:rsa-disj}).
%
\begin{examples}
\item\label{lisnorm}% 
  $\ListenerK(\state \given \msg)  = 
  \sum_{\Lex \in \LexSet} \ListenerK(\state, \Lex \given \msg)$
\item\label{spknorm}% 
  $\SpeakerK(\msg \given \state) \propto 
  \sum_{\Lex \in \LexSet} \SpeakerK(\msg \given \state, \Lex)
  \LexPrior(\Lex)$
\end{examples}

\begin{figure}[tp]
  \centering
  $\begin{array}[c]{c@{\hspace{45pt}}c}
     \ListenerK[1](\state\given\msg) & \SpeakerK[2](\msg\given\state) \\[1ex] % & \ListenerK[2](\state\given\msg) \\[1ex]
     \listenerMatrix%
     {}%
     { 0.29 & .71 & 0}
     { 1 &   0 &  0}
     {0 &  .25 & .75}
     & 
     \speakerMatrix%
     {}%
     {.23 &  .77  &   0}%
     {1   &  0  &     0}%
     {0   &  0.0  &   1}
     %
     % & 
     %
     %\listenerMatrix%
     %{}
     %{ .18 & .82  &  0}
     %{1 &   0  &  0}   
     %{0 &   0  &  1}      
   \end{array}$
   \caption{A return to simple signaling. The system on the left is
     derived from the middle column of \figref{fig:full-disj} using
     equation~\eg{lisnorm}, and the right column is derived from the
     right column of \figref{fig:full-disj} using
     equation~\eg{spknorm}.}
  \label{fig:simple}
\end{figure}


%=====================================================================

\section{Compositional disjunction}\label{sec:scalar-disj}

This initial example is meant to show how the model works and to show
that we can capture the core scalar implicature that disjunction is
best known for.

Close the space state under joins, so that, with three atomic worlds,
we get \figref{fig:closure}.

Assume that denotations are closed under joins in all lexica. If an
expressions basic denotation is $\set{w_{1}, w_{2}}$, then its full
extension is determined to be $\set{w_{1}, w_{2}, w_{1} \vee w_{2}}$.

The atomic messages are closed under disjunction and conjunction so
that $\Messages = \set{p, q}$ expands to
$\Messages = \set{p, q, \porq, \pandq}$. The semantics of these
operation is as expected: union for disjunction and intersection for
conjunction. Thus, for example, if $p$'s meaning is
$\set{w_{1}, w_{2}}$ and $q$'s meaning is $\set{w_{1}, w_{3}}$, then
$p \vee q$ denotes $\set{w_{1}, w_{2}, w_{3}}$.  After joint closure,
this denotes the entire state space represented in
\figref{fig:closure}.

\Figref{fig:compdisj} shows what happens when we run out full model in
this space. The diagrams summarize the listener and the speaker in
terms of the simple state/message signaling defined by \eg{lisnorm}
and \eg{spknorm}. The results again reveal the expected
exclusivization inferences for disjunction.

\begin{figure}[tp]
  \centering
  \newcommand{\labelednode}[2]{\put(#1){\makebox(0,0){#2}}}
  \newcommand{\picline}[3]{\put(#1){\line(#2){#3}}}
  \setlength{\unitlength}{1cm}
  \begin{picture}(5.5,2)   
    \labelednode{0.5,2}{$w_{1}$}
    \labelednode{2.75,2}{$w_{2}$}
    \labelednode{5,2}{$w_{2}$}
    
    \picline{0.5,1.2}{0,1}{0.6}
    \picline{0.75,1.2}{3,1}{1.8}
    \labelednode{0.5,1}{$w_{1} \vee w_{2}$}
        
    \picline{2.5,1.2}{-3,1}{1.8}
    \picline{3.0,1.2}{3,1}{1.8}
    \labelednode{2.75,1}{$w_{1} \vee w_{3}$}

    \picline{5,1.2}{0,1}{0.6}
    \picline{4.75,1.2}{-3,1}{1.8}
    \labelednode{5,1}{$w_{2} \vee w_{3}$}
    
    \picline{2.5,0.2}{-3,1}{1.8}
    \picline{2.75,0.2}{0,1}{0.6}
    \picline{3.0,0.2}{3,1}{1.8}
    \labelednode{2.75,0}{$w_{1} \vee w_{2} \vee w_{3}$}
  \end{picture}
  \caption{State space with disjunctive closure.}
  \label{fig:closure}
\end{figure}

\begin{figure}[tp]
  \begin{subfigure}{0.48\textwidth}
    \includegraphics[width=1\textwidth]{fig/scalardisj-expertise-listener-marginalized}
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.48\textwidth}
    \includegraphics[width=1\textwidth]{fig/scalardisj-expertise-speaker-lexsum}
  \end{subfigure}
  \caption{Compositional disjunction results. $\alpha = 3$; $\beta = 1$; $\Costs(\vee) = 1.0$; $n = 10$}
  \label{fig:compdisj}
\end{figure}


%=====================================================================

\section{Analysis}\label{sec:analysis}


Throughout, we
assume that the communcative setting is governed by a few overarching
principles: 

 communication about language is at least somewhat
important to the discourse participants; 

Contextual requirements: 

\begin{examples}
\item The speaker is mutually and publicly known to be an expert in
  the relevant domain.
  
\item The speaker is mutually and publicly known to believe the
  listener to be a non-expert in the relevant domain.
  
\item The speaker is mutually and publically known to have an
  interest in conveying information about the language itself,
  even if this communicative inefficiency in terms of the 
  information conveyed about the world.
\end{examples}

\begin{examples}
\item Throughout, let \word{X} be the unknown term.

\item From the listener's perspective, we are concerned to see when
  \word{A or X} gives rise to the inference that $\sem{A} \cap \sem{X}
  = \emptyset$ (Hurfordian reading) and when \word{A or X} gives rise
  to the inference that $\sem{A} = \sem{X}$ (definitional).
\end{examples}

\newcommand{\smallhurfordlex}[3]{
  \left[
    \begin{array}[c]{l@{ \ \mapsto \ }r l@{ \ \mapsto \ }r l@{ \ \mapsto \ }r}
      A & \set{#1} &
      B & \set{#2} &
      X & \set{#3}
    \end{array}
  \right]}

%=====================================================================

\subsection{Subsumptive disjunctions}\label{sec:analysis:subsumptive}

\begin{examples}
\item From the listener's perspective, we are concerned to see when
  \word{A or X} gives rise to the lexical inference that $\sem{A} \cap
  \sem{X} = \emptyset$. We assume that the overall meaning will be
  $\sem{A} \cup \sem{X}$, i.e., that the relevant information concerns
  just the lexical inference.

\item Here's a scenario in which the exclusivization inference arises:

  \begin{examples}
  \item $\States = \set{w_{1}, w_{2}, w_{3}}$;  $\Messages = \set{A, B, X}$
  \item $\Lex = [A \mapsto \set{w_{1}}, B \mapsto \set{w_{2}}, X \mapsto \set{w_{1}, w_{2}}]$
  \item Priors are flat. $\alpha = 2$; $\beta = 1$; $\gamma = 1$. Disjunction cost: $1$. $n = 3$. 
  \end{examples}

\item The listener's best guess inference, upon hearing \word{A or X},
  is that the speaker's state is $w_{1} \vee w_{2}$ and that the
  lexicon is the one where $A$ and $X$ are disjoint. Here's the joint
  probability table:
  \[
  \renewcommand{\arraystretch}{2}
  \begin{array}[c]{l r r r}
    \toprule
            & w_{1} & w_{2} & w_{1} \vee w_{2} \\
    \midrule
    \smallhurfordlex{w_{1}}{w_{2}}{w_{1}, w_{2}} & 0 & 0 & 0.16 \\
    \smallhurfordlex{w_{1}}{w_{2}}{w_{2}} & 0 & 0 & \graycell{0.47} \\
    \smallhurfordlex{w_{1}}{w_{2}}{w_{1}} & 0 & 0 & 0.38 \\
    \bottomrule
  \end{array}
  \]

\item The above parameters deliver the same result with the the
  lexicon size increased (same pattern: each lexical item denotes its
  own world and $X$ denotes the union of all the worlds). This is
  unwieldy to visualize, but the important thing is that the best
  lexicon is always a Hurfordian one. Here's the best lexical
  inference with five atomic messages and four atomic states:
  \[
  \left[
    \begin{array}[c]{l@{ \ \mapsto \ }l}
      A & \set{w_{1}} \\
      B & \set{w_{2}} \\
      C & \set{w_{3}} \\
      D & \set{w_{4}} \\
      X & \set{w_{2}, w_{3}, w_{4}}
    \end{array}
  \right]
  \]
  This is the `minimal' Hurfordian lexicon: the listener doesn't infer
  anything about $X$ except that it is disjoint from $A$.    
\end{examples}

%=====================================================================

\subsection{Definitional disjunctions}\label{sec:analysis:definitional}

\begin{examples}
\item From the listener's perspective, we are concerned to see when
  \word{A or X} is interpreted as equivalent to $\sem{A}$.

\item From the speaker's perspective, we want to know what happens
  when the speaker favors a lexicon in which $\sem{A}=\sem{X}$ and
  observes a state equivalent to the literal meaning of $\sem{A}$.
  When will such a speaker produce \word{A or X}.

\item Here's an example in which these two perspectives are
  complementary. To achieve it, we have to substantially raise
  $\alpha$ and $\beta$ and lower disjunction costs:

  \begin{examples}
  \item $\States = \set{w_{1}, w_{2}, w_{3}}$;  $\Messages = \set{A, B, X}$
  \item $\Lex = [A \mapsto \set{w_{1}}, B \mapsto \set{w_{2}}, X \mapsto \set{w_{1}, w_{2}}]$
  \item Priors are flat. $\alpha = 5$; $\beta = 7$; $\gamma = 1$. Disjunction cost: $0.01$. $n = 3$. 
  \end{examples}

\item The listener's best guess inference, upon hearing \word{A or X},
  is that the speaker's state is $w_{1} \vee w_{2}$ and that the
  lexicon is $\Lex_{1}$. Here's the joint probability table:
  \[
  \renewcommand{\arraystretch}{2}
  \begin{array}[c]{l r r r}
    \toprule
            & w_{1} & w_{2} & w_{1} \vee w_{2} \\
    \midrule
    \smallhurfordlex{w_{1}}{w_{2}}{w_{1}, w_{2}} & 0 & 0 & 0 \\
    \smallhurfordlex{w_{1}}{w_{2}}{w_{2}}        & 0 & 0 & 0 \\
    \smallhurfordlex{w_{1}}{w_{2}}{w_{1}}        & \graycell{0.88} & 0 & 0.12\\
    \bottomrule
  \end{array}
  \]

\item The above parameters deliver the same result with the the
  lexicon size increased. Here's the best lexical inference with five
  atomic messages and four atomic states --- in this way, the 
  listener has learned that $A$ and $X$ are synonymous:
  \[
  \left[
    \begin{array}[c]{l@{ \ \mapsto \ }l}
      A & \set{w_{1}} \\
      B & \set{w_{2}} \\
      C & \set{w_{3}} \\
      D & \set{w_{4}} \\
      X & \set{w_{1}}
    \end{array}
  \right]
  \]  

\item If we assume that the unknown term has an atomic meaning, then
  we can strengthen the inference (and generate it under a wider range
  of parameters settings).  Under these circumstances, the meaning of
  the known disjunct serves as a \emph{focal point} that the speaker
  and listener can coordinate on for the meaning of the unknown word.
\end{examples}

%=====================================================================

\subsection{Characterization}\label{sec:analysis:characterization}

\begin{examples}
\item The basic characterization is that definitional reading arise
  when disjunction costs are low and $\beta$ is high. Conversely,
  Hurfordian reading arise when disjunction cost are high and $\alpha$
  and $\beta$ are relatively close.
  
\item The intuition: where costs are high, the disjunction has to be
  justified. Letting the two terms overlap reduces the justification,
  whereas exclusivizing provides justification. In other words, the
  apparently undue prolixity of the disjunction (the more general term
  would seem to suffice!) generates an inference that we observe in
  the lexical inferences.
  
\item However, this needs to be qualified by the speaker's desire to
  communicate about the lexicon. If $\beta$ is high, then it might be
  worth paying the disjunction costs for the sake of teaching the
  listener about the lexicon, even if this involves a huge penalty in
  terms of informativity (since definitional readings convey only
  a single term's worth of information).

\item Earlier, we gave this charaterization of the contextual
  requirements for definitional readings. The boldfaced phrases
  connect these ideas with our model.

  \begin{examples}
  \item The speaker is mutually and publicly known to be an expert in
    the relevant domain. \textbf{The speaker observes a lexicon--state
      pair, and the listener seeks to figure out which one, which
      entails moving towards the speaker's lexicon.}      

  \item The speaker is mutually and publicly known to believe the
    listener to be a non-expert in the relevant domain.  \textbf{The
      speaker has lexical uncertainty --- it doesn't assume a lexicon
      but rather tries to infer one.}

  \item The speaker is mutually and publically known to have an
    interest in conveying information about the language itself, even
    if this communicative inefficiency in terms of the information
    conveyed about the world.  \textbf{High $\beta$, low disjunction
      costs.}
  \end{examples}

\item Additionally, the secondary nature of the definitional
  information (as compared with \word{oenophile means wine lover}) is
  captured by the fact that the inference is primarily about the
  lexicon, rather than about the information conveyed --- after all,
  absence the lexical inference, \word{A} would have done the same
  work with lower costs.

\item Hurfordian readings arise in a much wider range of parameters
  settings, that is contexts, because they survive high definitional
  costs are long as they can be justified.

\item Definitional readings exist mainly in the space of low
  disjunction and high $\beta$. This is more rarefied. We believe this
  is reflected in the data: definitional readings are relatively
  infrequent and delicate.

\item Here's a plot that does a good job of conveying the above.  It's
  for a relatively large lexicon: five atomic lexical items and five
  atomic states. (Smaller examples can be hard to interpret, since the
  model's overall pressure to achieve separating equilibria can
  create associations that we wouldn't expect to see in a more complex
  setting like, well, English.) The x-axis is $\log(\beta/\alpha)$, so
  $0.$ marks the points where $\beta = \alpha$. The y-axis is 
  the cost of disjunction (assuming $\gamma=1.0$).

  $\mspace{-80mu}$
  \includegraphics[width=1.2\textwidth]{fig/lex5-alpha-beta-gamma}

\end{examples}

%=====================================================================

\bibliographystyle{apalike}
\bibliography{levy-potts-pragdisj-bib}

\end{document}


